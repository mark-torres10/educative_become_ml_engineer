{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are useful for image recognition. In this section, we'll use the LeNet5 architecture, designed by Yann LeCun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "For the MNIST dataset, each image has dimensions 28 x 28 (784 pixels per image). So, each input image corresponds to a tensor of 784 normalized floating point values between 0.0 and 1.0. The label for an image is a one-hot tensor with 10 classes (0-9). \n",
    "\n",
    "When we use a batch of input data, the shape of `inputs` is (batch_size, self.input_dim * 2) (because there are batch_size images and each image is a square with length = input_dim) and the shape of the labels is (batch_size, self.output_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "batch_size = 16\n",
    "dataset = dataset.batch(batch_size)\n",
    "it = dataset.make_one_shot_iterator()\n",
    "inputs, labels = it.get_next()\n",
    "with tf.Session() as sess:\n",
    "    # Batch of data size 16\n",
    "    input_arr, label_arr = sess.run(\n",
    "        (inputs, labels))\n",
    "print(repr(input_arr))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        # CODE HERE\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "\n",
    "#### NHWC Format\n",
    "In Tf, image data has as NHWC format (num_samples, height, width, channels). So, we need to transform the 2-D data. For the MNIST dataset, the height and width of each image is self.input_dim, while the number of channels is 1. \n",
    "\n",
    "#### Reshaping the Data\n",
    "\n",
    "To reshape, we use tf.reshape. It takes in a tensor and new shape as required arguments. E.g., you could reshape a tensor from (4, 10) to (5, 4, 2) because both shapes contain 40 elements, but you could not reshape (4, 10) to (3, 10, 2). \n",
    "\n",
    "We can use -1 for one dimension of the new shape, and the dimension of -1 will take on the value necessary to allow the new shape to contain all the elements of the tensor. \n",
    "\n",
    "We reshape the data so we can set it up to run through the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    input_arr = sess.run(inputs)\n",
    "    reshaped_arr = sess.run(\n",
    "        tf.reshape(inputs, [-1, 2, 2, 1])\n",
    "    )\n",
    "print(repr(input_arr))\n",
    "print(repr(reshaped_arr))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        # reshaping the input in NHWC format\n",
    "        reshaped_inputs = tf.reshape(inputs, (-1, self.input_dim, self.input_dim, 1))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "#### Filters and kernels\n",
    "\n",
    "We use filters to transform inputs and extract features to recognize certain images. For example, we can have filters that magnify curves, so our model can tell the difference between curved digits (e.g., 8) and non-curved digits (e.g., 1). \n",
    "\n",
    "A kernel matrix defines the weights of a filter, and it is usually a square matrix. Like all neural network weights, the filter's weights are trainable. \n",
    "\n",
    "(so, in a feedforward neural net, you train the weights to create the linear combination of features that lead to the best prediction. You can apply that principle with CNNs to create the optimal filters to distinguish between digits by extracting the most useful hidden features)\n",
    "\n",
    "When the input data has multiple channels, a filter will have a separate kernel matrix per channel. \n",
    "\n",
    "#### Convolution\n",
    "\n",
    "The convolution represents how we combine our filter weights to the input data (so, we have the filters, which are like our \"weights\", and we figure the \"linear combination\" - or in this case, matrix product - of filter weights that give us the best outcome). \n",
    "\n",
    "The main operation used by a convolution is the matrix dot product (not matrix multiplication, but rather the sum over the element-wise product of two matrices. \n",
    "\n",
    "Ex. [[0.4, 0.1], [0.1, 0.2]] * [[0.0, 1.0], [0.4, 0.2]] = 0.18\n",
    "\n",
    "In addition to matrix dot products the convolution includes a trainable bias term that's added to the matrix dot product in a convolution. \n",
    "\n",
    "The number of matrix dot products in a convolution depends on the dimensions of the input data, the kernel matrix, and the stride size (vertical/horizontal offset of the kernel matrix as it moves along the input data). (so, when you pass the filter over the data, how far do you move left/right/up/down as you move from one portion of the input to the next). \n",
    "\n",
    "#### Padding\n",
    "\n",
    "We can change the size of the kernel matrix that we use (e.g., 2 x 2, 3 x 3, etc.). But, our combination of kernel and stride size might not fit nicely with the data. If we want to use all the input data in our convolution, we can pad the input matrix with 0s, adding rows/columns made entirely of 0s to the edges of the input data matrix. Since 0s multiplied by any number equals 0, the padding doesn't affect the matrix products. \n",
    "\n",
    "But, we only pad 0s to the right and bottom of the input data matrix, so as to avoid extra dot products and pad only the absolute minimum necessary to use all our input data. \n",
    "\n",
    "#### Convolution Layer\n",
    "\n",
    "A convolution layer (similar to a hidden layer in a feedforward network) applies multiple filters to the input tensor (and the neural network learns what the weights of these filters should be). While each filter has a separate kernel matrix for each of the input channels, the overall result of a filter's convolution is the sum of the convolutions across all the input channels. \n",
    "\n",
    "Adding more filters to a convolution layer allows the layer to better extract hidden features. But, this results in additional training time and computational complexity, since filters add extra weights to the model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        reshaped_inputs = tf.reshape(\n",
    "            inputs, [-1, self.input_dim, self.input_dim, 1])\n",
    "        # CODE HERE\n",
    "        conv1 = tf.layers.conv2d(reshaped_inputs, \n",
    "                                 filters = 32, \n",
    "                                 kernel_size = [5, 5], \n",
    "                                 padding = 'same', \n",
    "                                 activation = tf.nn.relu, \n",
    "                                 name = 'conv1')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "\n",
    "#### Purpose of Pooling\n",
    "\n",
    "The convolution layer extracts important hidden features, but the number of features it extracts can still be pretty large. To get around this, we can use $\\textit{pooling}$, which reduces the size of the data, allowing the model to perform fewer computations and train faster. (so, similar to dimensionality reduction in that sense)\n",
    "\n",
    "Pooling also prevents overfitting because it extracts only the most salient features and ignores distortions or uncommon features found only in a few examples.\n",
    "\n",
    "#### How Pooling Works\n",
    "\n",
    "We use filter matrices for pooling. But, the pooling filter doesn't have any weights, and it doesn't perform matrix dot products. Instead, it applies a reduction operation to subsections of the input data. \n",
    "\n",
    "(NOTE: Pooling can operate after a convolution step! It's used to reduce the size of the dataset, so after you convolve you can still have a pretty large matrix. You can then use pooling to reduce the dimensions, like you do with convolution, but you don't fundamentally create any values that weren't already there, like you do with convolution)\n",
    "\n",
    "The type of pooling usually used in CNNs is referred to as max pooling. The filters of max pooling use the \"max\" operation to obtain the maximum number in each submatrix of the input data. Below is an example: (using a 2 x 2 filter and a stride size of 1)\n",
    "\n",
    "[[1, 2, 5], [4, 0, 3], [0, 1, 9]] ---> [[4, 5], [4, 9]]\n",
    "\n",
    "Other types of pooling include min pooling and average pooling. \n",
    "\n",
    "For input data with dimensions $H_{in} x W_{in}$, the output of pooling with filter dimensions $H_F x W_F$ and stride size $S$ has the following height and weight:\n",
    "\n",
    "$$H_{out} = [\\frac{H_{in} - H_F - + 1}{S}]$$\n",
    "$$W_{out} = [\\frac{W_{in} - W_F - + 1}{S}]$$\n",
    "\n",
    "#### Padding\n",
    "\n",
    "Similar to convolutions, we may want to pad our input data prior to pooling. We pad our data with a value dependent on the pooling operation (e.g., for max pooling, we pad each matrix with $-\\infty$ (since $-\\infty$ is smaller than every number, it allows us to resize the input data without adding distortions when pooling)\n",
    "\n",
    "When padding, the output dimensions don't depend on the filter dimensions anymore. For input data with dimensions $H_{in} x W_{in}$, the output of padded pooling with a stride size of $S$ has dimensions $$H_{out} = [\\frac{H_{in}}{S}]$$ $$W_{out} = [\\frac{W_{in}}{S}]$$\n",
    "\n",
    "So, to clarify, the process, thus far, goes like this:\n",
    "\n",
    "Input --> \n",
    "\n",
    "Convolution (perform calculations, matrix dot products, sums, etc) --> \n",
    "\n",
    "(Max) Pooling (get subset of convolution values, to get dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        reshaped_inputs = tf.reshape(\n",
    "            inputs, [-1, self.input_dim, self.input_dim, 1])\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=reshaped_inputs,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "        # CODE HERE\n",
    "        pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size = [2, 2], strides = 2, name = 'pool1')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers\n",
    "\n",
    "We can increase the size and depth of a CNN\n",
    "\n",
    "#### Adding extra layers\n",
    "\n",
    "CNNs can benefit from additional layers. Additional layers allow a CNN to essentially stack multiple filters together for use on the image data. But, adding additional layers increases computational complexity, training time, and the risk of overfitting. \n",
    "\n",
    "For MNIST data, images are pretty simple and only have one channel, so one convolution and max pooling layer works well.\n",
    "\n",
    "#### Increased filters\n",
    "\n",
    "We normally increase the number of filters in a convolution layer the deeper the convolutional layer is in our model. This is because the deeper the convolution layer, the more detailed the extracted features become (e.g., the filters in the first layer might extract a feature like lines and edges, but the filters in the second layer take the combination of these features and extract more distinguished features, such as the sharp angle of a 7 or the intersecting curves of an 8). \n",
    "\n",
    "In the following example, our first convolution layer has 32 filters, while the second convolution layer has 64 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        reshaped_inputs = tf.reshape(\n",
    "            inputs, [-1, self.input_dim, self.input_dim, 1])\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=reshaped_inputs,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "        # Pooling Layer #1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs=conv1,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool1')\n",
    "        ####################\n",
    "        # Convolutional Layer #2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs = pool1, \n",
    "            filters=64, \n",
    "            kernel_size = [5, 5], \n",
    "            padding = 'same', \n",
    "            activation = tf.nn.relu, \n",
    "            name = 'conv2'\n",
    "        )\n",
    "        # Pooling Layer #2\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs = conv2, \n",
    "            pool_size = [2, 2], \n",
    "            strides = 2, \n",
    "            name = 'pool2'\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Fully-Connected Layers\n",
    "\n",
    "Fully-connected layers can be used to aggregate and flatten data\n",
    "\n",
    "#### Fully-connected layer\n",
    "\n",
    "After the output data of the second pooling layer, we can apply a fully-connected layer of size 1024 (the number of outputs that result from the second layer). The number of units is somewhat arbitrary - enough to be powerful, but not so much as to be resource-intensive. \n",
    "\n",
    "The purpose of the fully-connected layer is to aggregate the data features before we convert them to logits. This allows the model to make better predictions than if you directly convert the pooling output to logits (since you add an extra level of \"gathering\" that can combine outputs of different filters). \n",
    "\n",
    "#### Flattening\n",
    "\n",
    "The data, thus far, has been in NHWC format. But, in order to use a fully-connected layer, the data needs to be a matrix, where the # of rows = batch size, and # of columns = data features. \n",
    "\n",
    "We need to reshape, using tf.reshape, but in the opposite direction as before, now converting from NHWC to a 2-D matrix. \n",
    "\n",
    "Since the first dimension remains the batch size, we can use -1 for this dimension. To calculate the size of the second dimension (i.e., the total # of data features in `pool2`), we use the `shape` property of tensors in Tensorflow. The `shape` property gives us. `tf.TensorShape` object, which can be converted to a list of integers using its `as_list` function. \n",
    "\n",
    "The flattened data size is just the product of the H, W, and C sizes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Apply fully-connected layer\n",
    "    def create_fc(self, pool2):\n",
    "        # CODE HERE\n",
    "        pass\n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        reshaped_inputs = tf.reshape(\n",
    "            inputs, [-1, self.input_dim, self.input_dim, 1])\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=reshaped_inputs,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "        # Pooling Layer #1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs=conv1,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool1')\n",
    "        ##############\n",
    "        # Convolutional Layer #2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv2')\n",
    "        # Pooling Layer #2\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs=conv2,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool2')\n",
    "        dense = self.create_fc(pool2)\n",
    "        ##############\n",
    "        # get dimensions hwc (not including num_samples)\n",
    "        hwc = pool2.shape.as_list[1:]\n",
    "        # set value of flattened size\n",
    "        flattened_size = 1\n",
    "        for val in hwc:\n",
    "            flattened_size *= val        \n",
    "        # get flattened layer\n",
    "        pool2_flat = tf.reshape(pool2, (-1, flattened_size))\n",
    "        # apply a fully-connected (dense) layer\n",
    "        dense = tf.layers.dense(pool2_flat, 1024, activation = tf.nn.relu, name = 'dense')\n",
    "        # return dense layer\n",
    "        return dense\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "A problem with the CNN (and, frankly, any neural net) is the issue of overfitting. Dropout is one technique for reducing overfitting. \n",
    "\n",
    "#### Co-Adaptation\n",
    "\n",
    "Co-adaptation occurs when multiple neurons in a layer extract the same, or very similar, hidden features from the input data. This can happen when the connection weights for two different neurals are nearly identical. At that point, having the additional layers creates redundancy in the model. \n",
    "\n",
    "When a fully-connected layer has a large number of neurons, co-adapation is more likely to occur. This can be a problem for two reasons. First, it's a waste of computation. Second, if many neurons are extracting the same features, it magnifiies the significance of that one feature. This leads to overfitting if the duplicated feature(s) is/are specific to only the training set. \n",
    "\n",
    "#### Dropout\n",
    "\n",
    "To deal with the problem of co-adaptation, we can apply dropout during training. \n",
    "\n",
    "When we apply dropout, we randomly shut down some fraction of a layer's neurons at each training step (by zeroing out the neuron values). The fraction of neurons to be zero'd out is known as the dropout rate, $r_d$. The remaining neurons have their values multiplied by $\\frac{1}{1- r_d}$, so that the overall sum of the neuron values remains the same. \n",
    "\n",
    "By randomly dropping a fraction of the neurons, we're choosing a random sample of neurons to use at each training step. So, each individual neuron works with many different subsets of the other neurons rather than them all at once. This helps each neuron avoid depending to much on other neurons to correct its mistakes (the root cause of co-adaptation, since a neuron is relying on other neurons to capture the same features), while still allowing the neurons to learn different things from one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Apply dropout to final layer\n",
    "    def apply_dropout(self, dense, is_training):\n",
    "        # applies dropout to the dense fully-connected layer\n",
    "        dropout = tf.layers.dropout(dense, rate = 0.4, training = is_training)\n",
    "        return dropout\n",
    "        \n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        reshaped_inputs = tf.reshape(\n",
    "            inputs, [-1, self.input_dim, self.input_dim, 1])\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=reshaped_inputs,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "        # Pooling Layer #1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs=conv1,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool1')\n",
    "        # Convolutional Layer #2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv2')\n",
    "        # Pooling Layer #2\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs=conv2,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool2')\n",
    "        # Dense Layer\n",
    "        hwc = pool2.shape.as_list()[1:]\n",
    "        flattened_size = hwc[0] * hwc[1] * hwc[2]\n",
    "        pool2_flat = tf.reshape(pool2, [-1, flattened_size])\n",
    "        dense = tf.layers.dense(pool2_flat, 1024,\n",
    "            activation=tf.nn.relu, name='dense')\n",
    "        # Apply Dropout\n",
    "        dropout = self.apply_dropout(dense, is_training)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits\n",
    "\n",
    "Once we have a fully-connected layer, we can extract multiclass logits from the CNN\n",
    "\n",
    "#### Multiclass logits\n",
    "\n",
    "Since there are 10 possible digits an MNIST image can be, we use a 10-neuron fully-connected layer to obtain the logits for each digit class. \n",
    "\n",
    "Then, just like in multiclass classification, we can use a softmax to convert the logits to per-class probabilities. The labels are one-hot vectors, where the 'hot index' corresponds to the digit in the MNIST image. We can take the max probability to classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport tensorflow as tf\\n\\nclass MNISTModel(object):\\n    # Model Initialization\\n    def __init__(self, input_dim, output_size):\\n        self.input_dim = input_dim\\n        self.output_size = output_size\\n    \\n    # Get logits from the dropout layer\\n    def get_logits(self, dropout):\\n        # CODE HERE\\n        logits = tf.layers.dense(dropout, self.output_size, name = 'logits')\\n        return logits\\n    \\n    # CNN Layers\\n    def model_layers(self, inputs, is_training):\\n        reshaped_inputs = tf.reshape(\\n            inputs, [-1, self.input_dim, self.input_dim, 1])\\n        # Convolutional Layer #1\\n        conv1 = tf.layers.conv2d(\\n            inputs=reshaped_inputs,\\n            filters=32,\\n            kernel_size=[5, 5],\\n            padding='same',\\n            activation=tf.nn.relu,\\n            name='conv1')\\n        # Pooling Layer #1\\n        pool1 = tf.layers.max_pooling2d(\\n            inputs=conv1,\\n            pool_size=[2, 2],\\n            strides=2,\\n            name='pool1')\\n        # Convolutional Layer #2\\n        conv2 = tf.layers.conv2d(\\n            inputs=pool1,\\n            filters=64,\\n            kernel_size=[5, 5],\\n            padding='same',\\n            activation=tf.nn.relu,\\n            name='conv2')\\n        # Pooling Layer #2\\n        pool2 = tf.layers.max_pooling2d(\\n            inputs=conv2,\\n            pool_size=[2, 2],\\n            strides=2,\\n            name='pool2')\\n        # Dense Layer\\n        hwc = pool2.shape.as_list()[1:]\\n        flattened_size = hwc[0] * hwc[1] * hwc[2]\\n        pool2_flat = tf.reshape(pool2, [-1, flattened_size])\\n        dense = tf.layers.dense(pool2_flat, 1024,\\n            activation=tf.nn.relu, name='dense')\\n        # Apply Dropout\\n        dropout = tf.layers.dropout(dense, rate=0.4,\\n            training=is_training)\\n        # Get and Return Logits\\n        return self.get_logits(dropout)\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MNISTModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, input_dim, output_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Get logits from the dropout layer\n",
    "    def get_logits(self, dropout):\n",
    "        # CODE HERE\n",
    "        logits = tf.layers.dense(dropout, self.output_size, name = 'logits')\n",
    "        return logits\n",
    "    \n",
    "    # CNN Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        reshaped_inputs = tf.reshape(\n",
    "            inputs, [-1, self.input_dim, self.input_dim, 1])\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=reshaped_inputs,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "        # Pooling Layer #1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs=conv1,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool1')\n",
    "        # Convolutional Layer #2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv2')\n",
    "        # Pooling Layer #2\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs=conv2,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name='pool2')\n",
    "        # Dense Layer\n",
    "        hwc = pool2.shape.as_list()[1:]\n",
    "        flattened_size = hwc[0] * hwc[1] * hwc[2]\n",
    "        pool2_flat = tf.reshape(pool2, [-1, flattened_size])\n",
    "        dense = tf.layers.dense(pool2_flat, 1024,\n",
    "            activation=tf.nn.relu, name='dense')\n",
    "        # Apply Dropout\n",
    "        dropout = tf.layers.dropout(dense, rate=0.4,\n",
    "            training=is_training)\n",
    "        # Get and Return Logits\n",
    "        return self.get_logits(dropout)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### full code to get predictions\n",
    "\n",
    "\"\"\"\n",
    "def run_model_setup(self, inputs, labels, is_training):\n",
    "    logits = self.model_layers(inputs, is_training)\n",
    "\n",
    "    # convert logits to probabilities with softmax activation\n",
    "    self.probs = tf.nn.softmax(logits, name='probs')\n",
    "    # round probabilities\n",
    "    self.predictions = tf.argmax(\n",
    "        self.probs, axis=-1, name='predictions')\n",
    "    class_labels = tf.argmax(labels, axis=-1)\n",
    "    # find which predictions were correct\n",
    "    is_correct = tf.equal(\n",
    "        self.predictions, class_labels)\n",
    "    is_correct_float = tf.cast(\n",
    "        is_correct,\n",
    "        tf.float32)\n",
    "    # compute ratio of correct to incorrect predictions\n",
    "    self.accuracy = tf.reduce_mean(\n",
    "        is_correct_float)\n",
    "    # train model\n",
    "    if self.is_training:\n",
    "        labels_float = tf.cast(\n",
    "            labels, tf.float32)\n",
    "        # compute the loss using cross_entropy\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=labels_float,\n",
    "            logits=logits)\n",
    "        self.loss = tf.reduce_mean(\n",
    "            cross_entropy)\n",
    "        # use adam to train model\n",
    "        adam = tf.train.AdamOptimizer()\n",
    "        self.train_op = adam.minimize(\n",
    "            self.loss, global_step=self.global_step)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
