{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SqueezeNet\n",
    "\n",
    "SqueezeNet is a memory-efficient CNN that performs similarly to larger nets (e.g., AlexNet, the breakthrough CNN that won the 2012 ImageNet challenge).\n",
    "\n",
    "After training a model, we store its computational graph and parameters (weights + biases) for future use. The number of parameters can number in the millions, so it takes space to store them. \n",
    "\n",
    "Most high-performance models require hundreds of MB to store their parameters (e.g., AlexNet uses over 200MB for storage of 60 million parameters). But, SqueezeNet uses less than 1MB of space. \n",
    "\n",
    "The parameters that we'd need to store are the weights of each of the kernels, the number of kernels, the number of biases, and the number of input channels. For example, for a RGB image, there are 3 channels. Let's say there are two convolutional layers, and in each layer there are 3 kernels, each with dimensions 3 x 3. So, the # of parameters is ((3 x 3) (for the dimensions of a kernel) x 3 (number of kernels in a layer) x 2 (number of layers) + 2 (number of biases, one per layer)) x 3 (number of channels) = 168 parameters\n",
    "\n",
    "Having smaller models is helpful because it's faster to load and easier to store. \n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/1602.07360.pdf?source=post_page---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "In this example, we're using the CIFAR10 dataset. It contains 60,000 color images, with dimensions 32 x 32, evenly split across 10 categories. Since it's in RGB format, there are 3 channels, with each channel containing 1024 integers (32 x 32) between 0 and 255. So, an image is represented by 3072 integers between 0 and 255. \n",
    "\n",
    "This time (as opposed to MNIST), the output labels are indices of the classes, rather than one-hot representations. This is known as a \"sparse representation\" of the labels. \n",
    "\n",
    "For a batch of input data, the shape is (batch size, 3 x 32 x 32), and the shape of the labels is (batch size, ) since the labels are represented as a 1-D tensor. \n",
    "\n",
    "The images in CIFAR10 have the following labels: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "batch_size = 32\n",
    "dataset = dataset.batch(batch_size)\n",
    "it = dataset.make_one_shot_iterator()\n",
    "inputs_, labels = it.get_next()\n",
    "with tf.Session() as sess:\n",
    "    input_arr, label_arr = sess.run((inputs_, labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# beginning to build a SqueezeNet model class\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    def __init__(self, original_dim, resize_dim, output_size):\n",
    "        self.original_dim = original_dim\n",
    "        self.output_size = output_size\n",
    "        self.resize_dim = resize_dim\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "#### Data Augmentation\n",
    "\n",
    "For images, we can augment our data through image transformations. We can rotate and crop our images to make new images for our training sample. Using data augmentation, we create variations of each image in a batch of input data. Since we train our model here on a few hundred epochs, data augmentation constructs hundreds of variations for each image in a training set. \n",
    "\n",
    "#### Image Transformation\n",
    "\n",
    "For the CIFAR10 dataset, we'll use horizontal flips of the dataset. This is because for the types of images in CIFAR10, seeing a rotated version of them is unrealistic. \n",
    "\n",
    "Here, we crop each image to a randomly chosen submatrix of the data with height and width equal to a resized amount. Here, let's use resize_dim = 24 (originally, dim = 32). Then, with a probability of 0.5, we flip our image horizontally. \n",
    "\n",
    "Since we only apply data augmentation when training the model, we don't use image transformations when the model is not training. But, since we apply a random crop while training, our layers must be able to take in inputs with height and width = 24. Additionally, we need to resize our test images during evaluation. \n",
    "\n",
    "#### Data Standardization\n",
    "\n",
    "The CIFAR10 dataset has pixel integers between 0 and 255, and we need some way to standardize them. In the MNIST dataset, we standardized the pixel integers to between 0 and 1. Here instead, we use $\\textit {image standardization}$, in which, for a given image, we linearly scale the pixel data so that the data has 0 mean and unit variance (this is an improvement over the MNIST method because of the differences in color intensity, which could lead to differential impacts in backpropagation. For example, imagine an image that has a lot of strong red, blue, and green hues, as opposed to an image that has muted colors. Since all inputs will experience the same weights and biases, having weights and biases trained on images with strong hues (integer = 255) will mess up classification of those with more muted colors. Plus, it could lead to the gradient getting out of hand?). \n",
    "\n",
    "To accomplish image standardization, we use tf.image.per_image_standardization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class SqueezeNetModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, original_dim, resize_dim, output_size):\n",
    "        self.original_dim = original_dim\n",
    "        self.resize_dim = resize_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Random crop and flip\n",
    "    def random_crop_and_flip(self, float_image):\n",
    "        crop_image = tf.random_crop(float_image, [self.resize_dim, self.resize_dim, 3])\n",
    "        updated_image = tf.image.random_flip_left_right(crop_image)\n",
    "        return updated_image\n",
    "        \n",
    "    # Data Augmentation\n",
    "    def image_preprocessing(self, data, is_training):\n",
    "        reshaped_image = tf.reshape(data, [3, self.original_dim, self.original_dim])\n",
    "        transposed_image = tf.transpose(reshaped_image, [1, 2, 0])\n",
    "        float_image = tf.cast(transposed_image, tf.float32)\n",
    "        if is_training:\n",
    "            updated_image = self.random_crop_and_flip(float_image)\n",
    "        else:\n",
    "            updated_image = tf.image.resize_image_with_crop_or_pad(float_image, self.resize_dim, self.resize_dim)\n",
    "        standardized_image = tf.image.per_image_standardization(updated_image)\n",
    "        return standardized_image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fire Module\n",
    "\n",
    "The fire module is the central component of SqueezeNet\n",
    "\n",
    "#### Decreasing parameters\n",
    "\n",
    "In order to make a smaller model, we have to decrease the number of weights per convolutional layer. There are three ways to decrease the number of weights:\n",
    "1. Decrease the kernel size\n",
    "2. Decrease the number of kernel filters used\n",
    "3. Decrease the number of channels\n",
    "\n",
    "We want to keep the number of filters, since having a wide variety of filters allows us to extract different hidden features from the input. But, there are ways to decrease the kernel size and number of input channels while still maintaining good model performance. \n",
    "\n",
    "#### Kernel size (and how to decrease it)\n",
    "\n",
    "The kernel size represents the amount of spatial information it can capture. For example, a 1 x 1 kernel can only capture information from a single pixel, a 2 x 2 kernel can aggregate the information from four adjacent pixels, and a 3 x 3 kernel can aggregate the information between adjacent pixels in a 3 x 3 square of data. \n",
    "\n",
    "Larger kernels can capture more information (since they have a view of more information), it comes at the cost of additional parameters. A convolutional layer with 3 x 3 kernels will use 9x as many parameters as a layer that uses 1 x 1 kernels. A way to balance performance and parameter count is to use a mix of larger and smaller size kernels. \n",
    "\n",
    "#### Intermediate layer (used to decrease the number of input channels)\n",
    "\n",
    "An intermediate convolution layer can be used to decrease the number of input channels. Adding an extra layer introduces additional kernel weights, but it can drastically decrease the number of parameters used in a layer. \n",
    "\n",
    "For example, look at a convolutional layer with 100 filters and 3 x 3 kernels. If the input has 50 channels, the number of paramters = 3 x 3 x 100 x 50 + 100 = 45,100. \n",
    "\n",
    "But, if we first apply an intermediate convolutional layer with 10 filters and 1 x 1 kernels, the intermediate output will have 10 channels. The number of parameters used in the intermediate layer is 1 x 1 x 10 x 50 + 10 = 510. Then, if we pass the intermediate output into our original convolutional layer, the total number of parameters used becomes:\n",
    "510 + (3 x 3 x 100 x 10 + 100) = 9,610.\n",
    "\n",
    "So, we'd apply an intermediate convolution layer, which, here, reduces the number of filters from 50 to 10, leading to a 5-fold reduction in the number of parameters. \n",
    "\n",
    "#### Fire module\n",
    "\n",
    "The fire module, the key building block of SqueezeNet, applies concepts from the kernel size and intermediate layer sections. \n",
    "\n",
    "It uses an intermediate convolution layer, knowon as a \"squeeze layer\", then passes the intermediate output into an \"expand layer\" with a larger number of filters. \n",
    "\n",
    "The \"expand layer\" contains two convolution layers with an equal number of filters. One layer uses 1 x 1 kernels, while the other uses 3 x 3 kernels. Using 1 x 1 kernels helps decrease the number of parameters used. The outputs of the two layers have the same size, since both layers use the same number of filters. \n",
    "\n",
    "The two outputs are then concatenated along the channel dimension (which doubles the number of channels) to produce the overall output of the fire module. We use tf.concat to concatenate, and it has two required args: values and axis. Since the channel dimension is the last dimension, we use -1 for the axis. \n",
    "\n",
    "The ratio of the number of filters in the squeeze layer vs. the expand layer is known as the \"squeeze ratio\". A larger squeeze ratio (i.e., decreasing the number of filters in the squeeze layer) can improve the model performance up to a certain extent, at the cost of increasing the parameter count. \n",
    "\n",
    "Below is code for a fire module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class SqueezeNetModel(object):\n",
    "    # __init__ and other functions omitted\n",
    "\n",
    "    # Convolution layer wrapper\n",
    "    def custom_conv2d(self, inputs, filters, kernel_size, name):\n",
    "        return tf.layers.conv2d(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=tf.nn.relu,\n",
    "            padding='same',\n",
    "            name=name)\n",
    "\n",
    "    # SqueezeNet fire module\n",
    "    def fire_module(self, inputs, squeeze_depth, expand_depth, name):\n",
    "        with tf.variable_scope(name):\n",
    "            squeezed_inputs = self.custom_conv2d(\n",
    "                inputs,\n",
    "                squeeze_depth,\n",
    "                [1, 1],\n",
    "                'squeeze')\n",
    "            expand1x1 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [1, 1],\n",
    "                'expand1x1')\n",
    "            expand3x3 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [3, 3],\n",
    "                'expand3x3')\n",
    "            return tf.concat([expand1x1, expand3x3], axis=-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking fire modules\n",
    "\n",
    "We can stack multiple fire modules together by creating a utility function. \n",
    "\n",
    "#### Utility function\n",
    "\n",
    "The SqueezeNet model, though it has few parameters, still has many layers. There are several fire modules, which makes it useful to write a utility function that can stack multiple layers. \n",
    "\n",
    "When we deal with more complex model architectures, there are going to be repetitions of the main building blocks in the model. \n",
    "\n",
    "Below is a way to stack the modules together (essentially, through using for loops :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, original_dim, resize_dim, output_size):\n",
    "        self.original_dim = original_dim\n",
    "        self.resize_dim = resize_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Convolution layer wrapper\n",
    "    def custom_conv2d(self, inputs, filters, kernel_size, name):\n",
    "        return tf.layers.conv2d(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=tf.nn.relu,\n",
    "            padding='same',\n",
    "            name=name)\n",
    "\n",
    "    # SqueezeNet fire module\n",
    "    def fire_module(self, inputs, squeeze_depth, expand_depth, name):\n",
    "        with tf.variable_scope(name):\n",
    "            squeezed_inputs = self.custom_conv2d(\n",
    "                inputs,\n",
    "                squeeze_depth,\n",
    "                [1, 1],\n",
    "                'squeeze')\n",
    "            expand1x1 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [1, 1],\n",
    "                'expand1x1')\n",
    "            expand3x3 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [3, 3],\n",
    "                'expand3x3')\n",
    "            return tf.concat([expand1x1, expand3x3], axis=-1)\n",
    "    \n",
    "    # Stacked fire modules (use a for loop to iterate through each param list)\n",
    "    def multi_fire_module(self, layer, params_list):\n",
    "        for params in params_list:\n",
    "            layer = self.fire_module(layer, params[0], params[1], params[2])\n",
    "        return layer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layers\n",
    "\n",
    "Here, we discuss the high-level architecture of SqueezeNet\n",
    "\n",
    "#### Overview\n",
    "\n",
    "Now, we're building a condensed version of the SqueezeNet model. The differences between our model and the original SqueezeNet model are that (1) our model only uses the first 4 fire modules from the original SqueezeNet (rather than all 8), (2) the initial convolution layer for the model uses fewer filters (64 vs. 96) and a smaller kernel size (3x3 vs. 7x7), and (3) the max pooling layers in our model use smaller filters (2x2 vs. 3x3). \n",
    "\n",
    "#### Initial layers\n",
    "\n",
    "Here, we start with a regular convolution layer, rather than a fire module, to apply the initial filters to our image data. We do this so that we don't start off with squeezing our image data and thus risk losing on important general features.\n",
    "\n",
    "It's usually a good idea for any CNN to start off with a regular convolution layer. \n",
    "\n",
    "Moreover, it's typically a good idea to apply max pooling after the initial convolution layer. Reducing the dimension of the initial data helps our model train faster and pick up on important features. \n",
    "\n",
    "Again, we apply this convolution + max pooling before passing the data into any Fire modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, original_dim, resize_dim, output_size):\n",
    "        self.original_dim = original_dim\n",
    "        self.resize_dim = resize_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Convolution layer wrapper\n",
    "    def custom_conv2d(self, inputs, filters, kernel_size, name):\n",
    "        return tf.layers.conv2d(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=tf.nn.relu,\n",
    "            padding='same',\n",
    "            name=name)\n",
    "\n",
    "    # Max pooling layer wrapper\n",
    "    def custom_max_pooling2d(self, inputs, name):\n",
    "        return tf.layers.max_pooling2d(\n",
    "            inputs=inputs,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=name)\n",
    "    \n",
    "    # Model Layers\n",
    "    # inputs: [batch_size, resize_dim, resize_dim, 3]\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        conv1 = self.custom_conv2d(inputs, filters = 64, kernel_size = [3,3], name = 'conv1')\n",
    "        pool1 = self.custom_max_pooling2d(conv1, 'pool1')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Fire\n",
    "\n",
    "Here, we discuss adding Fire modules to the model and talk about delayed downsampling. \n",
    "\n",
    "#### Delayed downsampling\n",
    "\n",
    "Reducing the data dimensions via max pooling can help make our model training faster and more efficient, with performance similar to that of a model without max pooling.\n",
    "\n",
    "However, strategic placement of max pooling layers can improve the SqueezeNet model's accuracy. \n",
    "\n",
    "Normally, you apply max pooling after a convolutional layer, in order to reduce the dimensionality of the output.\n",
    "\n",
    "Here, we can, rather than applying max pooling after the squeeze layer in the first Fire module, instead apply it after the first two Fire modules. Placing the max pooling layers later in the module structure is known as $\\textit{delayed downsampling}$. We wait until later in the model architecture to downsample the data (i.e., reduce height/width dimensions through pooling), so that the earlier convolution layers can have a larger dimension input (and thus have more data to work with). \n",
    "\n",
    "The creators of SqueezeNet showed that having larger dimension inputs for more layers in the model can help improve accuracy, so there is a tradeoff between dimensionality and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, original_dim, resize_dim, output_size):\n",
    "        self.original_dim = original_dim\n",
    "        self.resize_dim = resize_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Convolution layer wrapper\n",
    "    def custom_conv2d(self, inputs, filters, kernel_size, name):\n",
    "        return tf.layers.conv2d(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=tf.nn.relu,\n",
    "            padding='same',\n",
    "            name=name)\n",
    "\n",
    "    # Max pooling layer wrapper\n",
    "    def custom_max_pooling2d(self, inputs, name):\n",
    "        return tf.layers.max_pooling2d(\n",
    "            inputs=inputs,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=name)\n",
    "    \n",
    "    # SqueezeNet fire module\n",
    "    def fire_module(self, inputs, squeeze_depth, expand_depth, name):\n",
    "        with tf.variable_scope(name):\n",
    "            squeezed_inputs = self.custom_conv2d(\n",
    "                inputs,\n",
    "                squeeze_depth,\n",
    "                [1, 1],\n",
    "                'squeeze')\n",
    "            expand1x1 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [1, 1],\n",
    "                'expand1x1')\n",
    "            expand3x3 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [3, 3],\n",
    "                'expand3x3')\n",
    "            return tf.concat([expand1x1, expand3x3], axis=-1)\n",
    "\n",
    "    # Utility function for multiple fire modules\n",
    "    def multi_fire_module(self, layer, params_list):\n",
    "        for params in params_list:\n",
    "            layer = self.fire_module(\n",
    "                layer,\n",
    "                params[0],\n",
    "                params[1],\n",
    "                params[2])\n",
    "        return layer\n",
    "    \n",
    "    # Model Layers\n",
    "    # inputs: [batch_size, resize_dim, resize_dim, 3]\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        conv1 = self.custom_conv2d(\n",
    "            inputs,\n",
    "            64,\n",
    "            [3, 3],\n",
    "            'conv1')\n",
    "        pool1 = self.custom_max_pooling2d(\n",
    "            conv1,\n",
    "            'pool1')\n",
    "        fire_params1 = ([32, 64,'fire1'],[32, 64,'fire2'])\n",
    "        # apply multi fire module blcok with list of tuple params\n",
    "        multi_fire1 = self.multi_fire_module(pool1, fire_params1)\n",
    "        # perform max pooling on the output of the multi-fire module block\n",
    "        pool2 = self.custom_max_pooling2d(multi_fire1, name = 'pool2')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth\n",
    "\n",
    "We can increase the SqueezeNet model's depth to improve performance.\n",
    "\n",
    "#### Deeper Fire modules\n",
    "\n",
    "In order to extract more distinguishing and nuanced features from the data, we can add more Fire modules to the model. The additional Fire modules will use twice as many filters in the expanded layer (from 64 to 128). This follows the same approach for normal CNNs, and it works because as you get deeper in the model, you're looking for more specific and nuanced features that are specific to a certain picture (e.g., a specific line in one corner of the image).\n",
    "\n",
    "To avoid overfitting, we apply dropout after the second multi-Fire moldule block, with a dropout rate of 0.5. We don't use a max pooling layer after this multi-Fire module block, since we can use average pooling to get logits. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mport tensorflow as tf\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    # __init__ and other functions omitted\n",
    "\n",
    "    # Model Layers\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        conv1 = self.custom_conv2d(\n",
    "            inputs,\n",
    "            64,\n",
    "            [3, 3],\n",
    "            'conv1')\n",
    "        pool1 = self.custom_max_pooling2d(\n",
    "            conv1,\n",
    "            'pool1')\n",
    "        fire_params1 = [\n",
    "            (32, 64, 'fire1'),\n",
    "            (32, 64, 'fire2')\n",
    "        ]\n",
    "        multi_fire1 = self.multi_fire_module(\n",
    "            pool1,\n",
    "            fire_params1)\n",
    "        ### Additional layers, for depth\n",
    "        pool2 = self.custom_max_pooling2d(\n",
    "            multi_fire1,\n",
    "            'pool2')\n",
    "        fire_params2 = [\n",
    "            (32, 128, 'fire3'),\n",
    "            (32, 128, 'fire4')\n",
    "        ]\n",
    "        multi_fire2 = self.multi_fire_module(\n",
    "            pool2,\n",
    "            fire_params2)\n",
    "        dropout1 = tf.layers.dropout(multi_fire2, rate=0.5,\n",
    "            training=is_training)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits\n",
    "\n",
    "We can use global average pooling to obtain the model's logits. \n",
    "\n",
    "#### Channel-based logits\n",
    "\n",
    "For the CNN section, we used flattening (because we had data in 3 channels that we needed in 2-D) and a couple of fully-connected layers (with dropout) to get the logits. Here, instead of flattening the data, we can instead use a convolution layer to convert the number of channels in our data to the # of classes. \n",
    "\n",
    "Then, we can use an average pooling layer to obtain logits for each channel (i.e., image class)\n",
    "\n",
    "#### Global average pooling\n",
    "\n",
    "Here, the filter performs an average on the inputs that it has. When the average pooling filter is the same height and width as the input data, it leads to a single output (average) per channel. This is known as \"global average pooling\"\n",
    "\n",
    "So, global average pooling gives us one metric, the average, for each channel. Each channel corresponds to a unique class, and the channel's averaged value represents the logit for that class. \n",
    "\n",
    "So it seems like, for example, one channel gives you the coefficients for how likely the image is to be a car, for example, and another channel gives you the coefficients for how likely the image is to be a cat. You can use global average pooling on each of these in order to see how likely it is that a given image is, for example, a cat.\n",
    "\n",
    "#### Advantages of using global pooling average (as opposed to fuly-connected layers)\n",
    "\n",
    "Using global average pooling is more native to CNN structure, since we obtain logits via channels rather than converting the data to flattened vectors. Because of this lack of conversion, the CNN can obtain more accurate logits for each image class. \n",
    "\n",
    "Moreover, since global average pooling is just a pooling layer it has no parameters. That means that there's no risk of overfitting the global average pooling layer. In contrast, fully-connected layers use many weight parameters, which risks overfitting (even when dropout diminishes the problem). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, original_dim, resize_dim, output_size):\n",
    "        self.original_dim = original_dim\n",
    "        self.resize_dim = resize_dim\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    # Convert final convolution layer to logits\n",
    "    def get_logits(self, conv_layer):\n",
    "        avg_pool1 = tf.layers.average_pooling2d(conv_layer, [conv_layer.shape[1], conv_layer.shape[2]], 1)\n",
    "        logits = tf.layers.flatten(avg_pool1, name = 'logits')\n",
    "        return logits\n",
    "        \n",
    "    # Convolution layer wrapper\n",
    "    def custom_conv2d(self, inputs, filters, kernel_size, name):\n",
    "        return tf.layers.conv2d(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=tf.nn.relu,\n",
    "            padding='same',\n",
    "            name=name)\n",
    "\n",
    "    # Max pooling layer wrapper\n",
    "    def custom_max_pooling2d(self, inputs, name):\n",
    "        return tf.layers.max_pooling2d(\n",
    "            inputs=inputs,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=name)\n",
    "    \n",
    "    # SqueezeNet fire module\n",
    "    def fire_module(self, inputs, squeeze_depth, expand_depth, name):\n",
    "        with tf.variable_scope(name):\n",
    "            squeezed_inputs = self.custom_conv2d(\n",
    "                inputs,\n",
    "                squeeze_depth,\n",
    "                [1, 1],\n",
    "                'squeeze')\n",
    "            expand1x1 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [1, 1],\n",
    "                'expand1x1')\n",
    "            expand3x3 = self.custom_conv2d(\n",
    "                squeezed_inputs,\n",
    "                expand_depth,\n",
    "                [3, 3],\n",
    "                'expand3x3')\n",
    "            return tf.concat([expand1x1, expand3x3], axis=-1)\n",
    "\n",
    "    # Utility function for multiple fire modules\n",
    "    def multi_fire_module(self, layer, params_list):\n",
    "        for params in params_list:\n",
    "            layer = self.fire_module(\n",
    "                layer,\n",
    "                params[0],\n",
    "                params[1],\n",
    "                params[2])\n",
    "        return layer\n",
    "    \n",
    "    # Model Layers\n",
    "    # inputs: [batch_size, resize_dim, resize_dim, 3]\n",
    "    def model_layers(self, inputs, is_training):\n",
    "        conv1 = self.custom_conv2d(\n",
    "            inputs,\n",
    "            64,\n",
    "            [3, 3],\n",
    "            'conv1')\n",
    "        pool1 = self.custom_max_pooling2d(\n",
    "            conv1,\n",
    "            'pool1')\n",
    "        fire_params1 = [\n",
    "            (32, 64, 'fire1'),\n",
    "            (32, 64, 'fire2')\n",
    "        ]\n",
    "        multi_fire1 = self.multi_fire_module(\n",
    "            pool1,\n",
    "            fire_params1)\n",
    "        pool2 = self.custom_max_pooling2d(\n",
    "            multi_fire1,\n",
    "            'pool2')\n",
    "        fire_params2 = [\n",
    "            (32, 128, 'fire3'),\n",
    "            (32, 128, 'fire4')\n",
    "        ]\n",
    "        multi_fire2 = self.multi_fire_module(\n",
    "            pool2,\n",
    "            fire_params2)\n",
    "        dropout1 = tf.layers.dropout(multi_fire2, rate=0.5,\n",
    "            training=is_training)\n",
    "        final_conv_layer = self.custom_conv2d(\n",
    "            dropout1,\n",
    "            self.output_size,\n",
    "            [1, 1],\n",
    "            'final_conv')\n",
    "        return self.get_logits(final_conv_layer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Labels\n",
    "\n",
    "#### Sparse representation\n",
    "\n",
    "We use sparse representation for the CIFAR-10 labels. Instead of being one-hot vectors, each label is just the index of its corresponding image class. Using this sparse representation saves space compared to one-hot representation.\n",
    "\n",
    "For training the model, we can use a sparse version of softmax in order to get the classes. In Tensorflow, this is provided through the tf.nn.sparse_softmax_cross_entropy_with_logits() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class SqueezeNetModel(object):\n",
    "    # __init__ and other functions omitted\n",
    "\n",
    "    # Set up and run model training\n",
    "    def run_model_setup(self, inputs, labels):\n",
    "      logits = self.model_layers(inputs, is_training)\n",
    "      self.probs = tf.nn.softmax(logits, name='probs')\n",
    "      self.predictions = tf.argmax(\n",
    "          self.probs, axis=-1, name='predictions')\n",
    "      is_correct = tf.equal(\n",
    "          tf.cast(self.predictions, tf.int32),\n",
    "          labels)\n",
    "      is_correct_float = tf.cast(\n",
    "          is_correct,\n",
    "          tf.float32)\n",
    "      self.accuracy = tf.reduce_mean(\n",
    "          is_correct_float)\n",
    "      # calculate cross entropy\n",
    "      cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=labels,\n",
    "          logits=logits)\n",
    "      self.loss = tf.reduce_mean(\n",
    "          cross_entropy)\n",
    "      adam = tf.train.AdamOptimizer()\n",
    "      self.train_op = adam.minimize(\n",
    "          self.loss, global_step=self.global_step)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
