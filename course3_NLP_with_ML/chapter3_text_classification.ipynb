{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "In this section, we'll build a bidirectional LSTM for classification\n",
    "\n",
    "#### Classifying text\n",
    "\n",
    "An important task in NLP is to classify text into different categories. Some uses of this include spam filtering, classifying user product reviews, and automatically flagging inappropriate or harmful social media posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "#### Classifying sentiment\n",
    "\n",
    "We can classify sentiment of text (i.e., the writer's attitude) using multiclass classification (e.g., figure out their emotion along a scale) or binary classification (e.g., \"positive\" vs. \"negative\")\n",
    "\n",
    "#### Training pairs\n",
    "\n",
    "Our input data is tokenized text sequences, and each text sequence will be labeled with a class/category.\n",
    "\n",
    "For example: \n",
    "\n",
    "1. ([1, 5, 6, 8, 2], 0)\n",
    "2. ([3, 5, 2, 9, 8], 1)\n",
    "\n",
    "These are two examples of training tuples, where the first entry in each tuple is the tokenized text IDs and the last entry is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "\n",
    "# Text classification model\n",
    "class ClassificationModel(object):\n",
    "    # Model initialization\n",
    "    def __init__(self, vocab_size, max_length, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    def tokenize_text_corpus(self, texts):\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        return sequences\n",
    "    \n",
    "    # Create training pairs for text classification\n",
    "    def make_training_pairs(self, texts, labels):\n",
    "        sequences = self.tokenize_text_corpus(texts)\n",
    "        for i in range(len(sequences)):\n",
    "            sequence = sequences[i]\n",
    "            if len(sequence) > self.max_length:\n",
    "                sequences[i] = sequence[:self.max_length]\n",
    "        training_pairs = list(zip(sequences, labels))\n",
    "        return training_pairs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "We can use TensorFlow feature columns to create input embeddings\n",
    "\n",
    "#### Feature columns\n",
    "\n",
    "Prior approaches covered pre-training an embedding model as well as training an embedding model in tandem with an LSTM. In both of these cases, we had to write our own embedding matrix and handle its initialization.\n",
    "\n",
    "We can instead use TensorFlow's feature column API. In particular, we can use the `tf.feature_column.embedding_column` function, which lets you incorporate an embedding matrix automatically into the model, which will be trained alongside the LSTM.\n",
    "\n",
    "#### Sequential categorial column\n",
    "\n",
    "The `embedding_column` function takes in two required arguments. The second argument is the embedding size, which is typically set to be the 4th root of the vocabulary size. The first argument is a \"categorical column\", and for functions that work with sequential data, we have to use `tf.contrib.feature_column`, which comes from the extended feature column API. Since each of the vocabulary in the text corpus is converted to a unique integer, we can use the sequece_categorical_column_with_identity function. It takes in two arguments: a string name for the categorical colummn and the vocabulary size of the text corpus. The string name will be used when creating the input dictionary for the main conversion function. Categorical columns don't contain any data until a computational graph is run (just like tf.placeholder objects)\n",
    "\n",
    "Below is some code that creates an embedding column, \"embed_col\", using a categorical column, \"input_col\" as input.\n",
    "\n",
    "`\n",
    "import tensorflow as tf\n",
    "vocab_size = 10000\n",
    "input_col = tf.contrib.feature_column \\\n",
    "              .sequence_categorical_column_with_identity(\n",
    "                  'input', vocab_size)\n",
    "embed_size = int(10000**0.25)\n",
    "embed_col = tf.feature_column.embedding_column(\n",
    "                  input_col, embed_size)\n",
    "`\n",
    "\n",
    "#### Converting to embeddings\n",
    "\n",
    "The main conversion function used to create the embedded input sequences is `sequence_input_layer`. It takes two arguments: (1) a dictionary of input data, where each key is the name of a categorical column, and (2) a list of feature columns corresponding to data in the input dictionary. \n",
    "\n",
    "Since the input data is just a batch of tokenized sequences, the dictionary will only contain a single key-value pair. The key is the same string set in the `sequence_categorical_column_with_identity` function. The second argument will just be a list containing the embedding column.\n",
    "\n",
    "Below is an example of `sequence_input_layer` with tokenized sequences (\"input_seqs\") and embed_col from above:\n",
    "\n",
    "`\n",
    "import tensorflow as tf\n",
    "input_seqs = tf.placeholder(tf.int64, shape=(None, 30)) # thirty time steps\n",
    "input_dict = {'input': input_seqs}\n",
    "embed_seqs, sequence_lengths = tf.contrib.feature_column \\\n",
    "                                 .sequence_input_layer(\n",
    "                                     input_dict, [embed_col])\n",
    "`\n",
    "\n",
    "The output of `sequence_input_layer` is a tuple containing the embedded sequences and the sequence lengths. The sequence lengths output is used for calculating the sequence lengths for variable length sequence inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "#### Forwards and bckwards\n",
    "\n",
    "When we have access to a completed text sequence (e.g., text classification, as opposed to predicting the next word in a sentence), it might be beneficial to look at the sequence in both the forwards and backwards directions. \n",
    "\n",
    "#### Bidirectional LSTM (BiLSTM)\n",
    "\n",
    "A bidirectional LSTM is just model which has both a forward LSTM and a backwards LSTM (which reads the input sequence in reverse)\n",
    "\n",
    "In Tensorflow, we can use the `tf.nn.bidirectional_dynamic_rnn` to create a bidirectional LSTM. It runs similar to the `tf.nn.dynamic_rnn`, except it takes in two LSTM cells rather than one. See below for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "cell_fw = tf.nn.rnn_cell.LSTMCell(7)\n",
    "cell_bw = tf.nn.rnn_cell.LSTMCell(7)\n",
    "\n",
    "# Embedded input sequences\n",
    "# Shape: (batch_size, time_steps, embed_dim)\n",
    "input_embeddings = tf.placeholder(\n",
    "    tf.float32, shape=(None, 10, 12))\n",
    "outputs, final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw,\n",
    "    cell_bw,\n",
    "    input_embeddings,\n",
    "    dtype=tf.float32)\n",
    "print(outputs[0])\n",
    "print(outputs[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.bidirectional_dynamic_rnn` function returns a tuple containing the LSTM outputs and final LSTM states. Since a BiLSTM contains two LSTMs, both `outputs`and `final_states` are tuples. `outputs[0]`represents the outputs of the forward LSTM, while `outputs[1]` represents the outputs of the backwards LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "\n",
    "# Text classification model\n",
    "class ClassificationModel(object):\n",
    "    # Model initialization\n",
    "    def __init__(self, vocab_size, max_length, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    # Make LSTM cell with dropout\n",
    "    def make_lstm_cell(self, dropout_keep_prob):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.num_lstm_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Use feature columns to create input embeddings\n",
    "    def get_input_embeddings(self, input_sequences):\n",
    "        inputs_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "            'inputs',\n",
    "            self.vocab_size)\n",
    "        embedding_column = tf.feature_column.embedding_column(\n",
    "            inputs_column,\n",
    "            int(self.vocab_size**0.25))\n",
    "        inputs_dict = {'inputs': input_sequences}\n",
    "        input_embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "            inputs_dict,\n",
    "            [embedding_column])\n",
    "        return input_embeddings, sequence_lengths\n",
    "    \n",
    "    # Create and run a BiLSTM on the input sequences\n",
    "    def run_bilstm(self, input_sequences, is_training):\n",
    "        input_embeddings, sequence_lengths = self.get_input_embeddings(input_sequences)\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_fw = self.make_lstm_cell(dropout_keep_prob)\n",
    "        cell_bw = self.make_lstm_cell(dropout_keep_prob)\n",
    "        lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_embeddings, sequence_length = sequence_lengths, dtype = tf.float32)\n",
    "        return (lstm_outputs, sequence_lengths)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits\n",
    "\n",
    "How can we calculate logits from a BiLSTM model?\n",
    "\n",
    "#### Concatenation\n",
    "\n",
    "The BiLSTM gives us two outputs, the forwards and backwards outputs. To calculate the logits, we need to combine the outputs through concatenation, using `tf.concat`, which allows us to concatenate a list of tensors, along a particular dimension. \n",
    "\n",
    "Below is an example of `tf.concat`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "# Shape: (2, 2, 3)\n",
    "t1 = tf.constant([\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[0, 4, 8], [3, 2, 2]]\n",
    "])\n",
    "\n",
    "# Shape: (1, 2, 3)\n",
    "t2 = tf.constant([\n",
    "    [[9, 9, 9], [8, 8, 8]]\n",
    "])\n",
    "\n",
    "# Shape: (2, 2, 2)\n",
    "t3 = tf.constant([\n",
    "    [[9, 9], [1, 1]],\n",
    "    [[7, 2], [8, 8]]\n",
    "])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    o1 = sess.run(tf.concat([t1, t2], 0))\n",
    "    o2 = sess.run(tf.concat([t1, t3], -1))\n",
    "\n",
    "print(repr(o1))\n",
    "print(repr(o2))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "\n",
    "array([[[1, 2, 3],\n",
    "        [4, 5, 6]],\n",
    "\n",
    "       [[0, 4, 8],\n",
    "        [3, 2, 2]],\n",
    "\n",
    "       [[9, 9, 9],\n",
    "        [8, 8, 8]]], dtype=int32)\n",
    "array([[[1, 2, 3, 9, 9],\n",
    "        [4, 5, 6, 1, 1]],\n",
    "\n",
    "       [[0, 4, 8, 7, 2],\n",
    "        [3, 2, 2, 8, 8]]], dtype=int32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When concatenating tensors, the tensors need to have the exact same shape, except for the axis that's being concatenated. \n",
    "\n",
    "A quick way to do this is to set -1 for the second argument, to specify the final tensor dimension as the axis of concatenation. \n",
    "\n",
    "#### Final time step\n",
    "\n",
    "When we create an LSTM for classification, we only use the final time step output for each sequence in the data  (since we want to take in the prediction after the model has seen every word, as opposed to the predictions from last chapter, where we got a prediction after each word because we wanted to complete partial sequecnes). \n",
    "\n",
    "So, after combining the forwards and backwards LSTM outputs, we retrieve the final time step values using `tf.gather_nd` to get the final value. We then pass those values through a final fully-connected layer in order to get the logits\n",
    "\n",
    "Below, we create a `calculate_logits` function, which calculates logits based on the outputs of the BiLSTM\n",
    "\n",
    "The input, `lstm_outputs` is a tuple containing the outputs of the forwards and backwards LSTMs. We first need to separate the tuple into two distinct variables. Then, we concatenate the output values along their final dimension. We calculate the indices of each sequence's final time step, and use `tf.gather_nd` to retrieve said final time step. Since our task is binary text classification, we use a final fully-connected layer with a single node, in order to obtain the model's logits (since we'll have multipe outputs, which we need to aggregate into a single class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "\n",
    "# Text classification model\n",
    "class ClassificationModel(object):\n",
    "    # Model initialization\n",
    "    def __init__(self, vocab_size, max_length, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    def get_gather_indices(self, batch_size, sequence_lengths):\n",
    "        row_indices = tf.range(batch_size)\n",
    "        final_indexes = tf.cast(sequence_lengths - 1, tf.int32)\n",
    "        return tf.transpose([row_indices, final_indexes])\n",
    "\n",
    "    # Calculate logits based on the outputs of the BiLSTM\n",
    "    def calculate_logits(self, lstm_outputs, batch_size, sequence_lengths):\n",
    "        lstm_outputs_fw, lstm_outputs_bw = lstm_outputs\n",
    "        combined_outputs = tf.concat([lstm_outputs_fw, lstm_outputs_bw], -1)\n",
    "        gather_indices = self.get_gather_indices(batch_size, sequence_lengths)\n",
    "        final_outputs = tf.gather_nd(combined_outputs, gather_indices)\n",
    "        logits = tf.layers.dense(final_outputs, 1)\n",
    "        return logits\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "We can calculate the model's loss using sigmoid cross entropy, using the code below:\n",
    "\n",
    "We first calculate the logits (using the code above). Then, we use sigmoid cross entropy for the loss (and convert the integer labels into floats)\n",
    "\n",
    "The output of the function is the overall aggregate loss, so we need to sum each individual sequence's loss in the batch.\n",
    "\n",
    "To complete our prediction, we can just round to the nearest integer (so, 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "\n",
    "# Text classification model\n",
    "class ClassificationModel(object):\n",
    "    # Model initialization\n",
    "    def __init__(self, vocab_size, max_length, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "    def get_gather_indices(self, batch_size, sequence_lengths):\n",
    "        row_indices = tf.range(batch_size)\n",
    "        final_indexes = tf.cast(sequence_lengths - 1, tf.int32)\n",
    "        return tf.transpose([row_indices, final_indexes])\n",
    "\n",
    "    # Calculate logits based on the outputs of the BiLSTM\n",
    "    def calculate_logits(self, lstm_outputs, batch_size, sequence_lengths):\n",
    "        lstm_outputs_fw, lstm_outputs_bw = lstm_outputs\n",
    "        combined_outputs = tf.concat([lstm_outputs_fw, lstm_outputs_bw], -1)\n",
    "        gather_indices = self.get_gather_indices(batch_size, sequence_lengths)\n",
    "        final_outputs = tf.gather_nd(combined_outputs, gather_indices)\n",
    "        logits = tf.layers.dense(final_outputs, 1)\n",
    "        return logits\n",
    "    \n",
    "    # Calculate the loss for the BiLSTM\n",
    "    def calculate_loss(self, lstm_outputs, batch_size, sequence_lengths, labels):\n",
    "        logits = self.calculate_logits(lstm_outputs, batch_size, sequence_lengths)\n",
    "        float_labels = tf.cast(labels, tf.float32)\n",
    "        batch_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = float_labels, logits = logits)\n",
    "        overall_loss = tf.reduce_sum(batch_loss)\n",
    "        return overall_loss\n",
    "    \n",
    "    # Convert logits to predictions\n",
    "    def logits_to_predictions(self, logits):\n",
    "        probs = tf.nn.sigmoid(logits)\n",
    "        preds = tf.round(probs)\n",
    "        return preds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Model Performance\n",
    "\n",
    "To improve BiLSTM performance, we can perhaps use more LSTM layers or hidden LSTM units (with the caveat that we need to be sure not to overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
