{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "Here, we're building a sequence-to-sequence (seq2seq) model, which is used for tasks that involve reading in a sequence of text and generating an output text sequence based on the input. \n",
    "\n",
    "#### Sequence to sequence\n",
    "\n",
    "The sequence to sequence (seq2seq) framework encompasses any task that involves taking in some text and returning some generated text. Some examples of this include chatbots, text summarization, and machine translation. \n",
    "\n",
    "In the past, these seq2seq tasks were often performed using Bayesian statistics. But, we've been able to apply deep learning to these tasks.\n",
    "\n",
    "In particular, there's an extremely powerful model called the \"encoder-decoder\", which is specifically designed for seq2seq applications.\n",
    "\n",
    "The encoder-decoder is named for its two parts: the encoder and the decoder. Both the encoder and decoder are langauge models. \n",
    "\n",
    "First, an input sequence is fed to the encoder. The output from the last layer of the encoder becomes the input for the first layer of the decoder. The decoder transforms that input back into a text sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Seq2Seq (from paper)\n",
    "\n",
    "See this paper: (https://arxiv.org/pdf/1409.3215v3.pdf)\n",
    "See this tutorial: (https://www.youtube.com/watch?v=ElmBrKyMXxs)\n",
    "\n",
    "When you train a Seq2Seq model, you have an encoder network and a decoder network. You split a sentence such as \"I know that all dogs are really good pets\" into something like \"I know that all dogs\" and \"are really good pets\".\n",
    "\n",
    "In the encoding portion, you run \"I know that all dogs\" and get the output and a hidden state. You feed these (not sure about output, but at the very least, you feed in the hidden state) into a decoder, which will try to recreate \"are really good pets\". \n",
    "\n",
    "As you do this over and over, the decoder output should better approximate the true output sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "#### Training task\n",
    "\n",
    "For a seq2seq model, we use training pairs that contain an input sequence and an output sequence (and the goal is to predict the output sequence).\n",
    "\n",
    "During training, we perform two tasks:\n",
    "1. **Input Task**: Extract useful information from the input sequence\n",
    "2. **Output Task**: Calculate word probabilities at each output time step, using information from the input sequence and **previous** words in the output sequence.\n",
    "\n",
    "We can leave the input sequence as is. We process the output sequence into two separate sequences: the ground truth sequence and the final token sequence.\n",
    "\n",
    "#### Processing the output\n",
    "\n",
    "The ground truth sequence for a seq2seq model is equal to the input sequence for a language model - it represents sequence prefixes that we use to calculate word probabilities at each time step of the output. \n",
    "\n",
    "#### SOS and EOS tokens\n",
    "\n",
    "For seq2seq models, we need to have start-of-sequence (SOS) and end-of-sequence (EOS) tokens, which mark the start and end of a tokenized text sequence. \n",
    "\n",
    "Example. ['SOS', 'he', 'eats', 'bread', 'EOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    # Create a sequence training tuple from input/output sequences\n",
    "    def make_training_tuple(self, input_sequence, output_sequence):\n",
    "        truncate_front = output_sequence[1:]\n",
    "        truncate_back = output_sequence[:-1]\n",
    "        sos_token = [self.vocab_size]\n",
    "        eos_token = [self.vocab_size + 1]\n",
    "        # create input, output sequences\n",
    "        input_sequence = [sos_token] + input_sequence + [eos_token]\n",
    "        ground_truth = truncate_back + [sos_token]\n",
    "        final_sequence = truncate_front + [eos_token]\n",
    "        return (input_sequence, ground_truth, final_sequence)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final States\n",
    "\n",
    "Here, let's compare the final state output of an LSTM and BiLSTM\n",
    "\n",
    "#### The encoder\n",
    "\n",
    "The encoder is used to extract useful information, and is typically an LSTM or BiLSTM\n",
    "\n",
    "#### LSTM final state\n",
    "\n",
    "We pass the final state of the encoder into the decoder. For an LSTM in Tensorflow, the final state is represented by the LSTMStateTuple object, which contains two important properties: (1) the hidden state (c) and (2) the state output (h). The hidden state represents the internal cell state (\"memory\") of the LSTM cell. \n",
    "\n",
    "The two properties are represented by tensors with shape = (batch_size, hidden_units)\n",
    "\n",
    "#### Multi-layer final states\n",
    "\n",
    "For a multi-layer LSTM, the final state output of `dynamic_rnn` is a tuple containing the final state for each layer.\n",
    "\n",
    "#### BiLSTM final state\n",
    "\n",
    "The final state of a BiLSTM is similar to that of the LSTM, except the output is a tuple of two LSTMStateTuple objects (one for the forward LSTM and one for the backward LSTM)\n",
    "\n",
    "#### Combining forward and backward, for BiLSTM\n",
    "\n",
    "Since the decoder portion utilizes only a regular LSTM (which works in the forward direction), we need to combine the forward and backward LSTMs, through concatenating the hidden state and state output of both the forward and backward states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Get c and h vectors for bidirectional LSTM final states\n",
    "def get_bi_state_parts(state_fw, state_bw):\n",
    "    bi_state_c = tf.concat([state_fw.c, state_bw.c],-1)\n",
    "    bi_state_h = tf.concat([state_fw.h, state_bw.h], -1)\n",
    "    return (bi_state_c, bi_state_h)\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Create multi-layer LSTM\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "\n",
    "    # Get embeddings for input/output sequences\n",
    "    def get_embeddings(self, sequences, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            cat_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "                'sequences',\n",
    "                self.extended_vocab_size)\n",
    "            embedding_column = tf.feature_column.embedding_column(\n",
    "                cat_column,\n",
    "                int(self.extended_vocab_size**0.25))\n",
    "            seq_dict = {'sequences': sequences}\n",
    "            embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "                seq_dict,\n",
    "                [embedding_column])\n",
    "            return embeddings, tf.cast(sequence_lengths, tf.int32)\n",
    "    \n",
    "    # Create the encoder for the model\n",
    "    def encoder(self, encoder_inputs, is_training):\n",
    "        input_embeddings, input_seq_lens = self.get_embeddings(encoder_inputs, 'encoder_emb')\n",
    "        cell_fw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        cell_bw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        enc_outputs, final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            input_embeddings,\n",
    "            sequence_length=input_seq_lens,\n",
    "            dtype=tf.float32)\n",
    "        states_fw, states_bw = final_states\n",
    "        combined_state = []\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            bi_state_c, bi_state_h = get_bi_state_parts(\n",
    "                states_fw[i], states_bw[i]\n",
    "            )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined State\n",
    "\n",
    "We need to combine the final states for a BiLSTM into usable initial states\n",
    "\n",
    "#### LSTMStateTuple initialization\n",
    "\n",
    "We can initialize an LSTMStateTuple object with a hidden state (c) and state output (h). \n",
    "\n",
    "Below is an example of combining the BiLSTM forward and backward states into a single LSTMStateTuple object, which can be passed into the decoder. \n",
    "\n",
    "For BiLSTM encoders with multiple layers, we combine the states for each layer to create a tuple of `LSTMStateTuple` objects, where the element at index \"i\" of the tuple is the \"ith\" layer's combined final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Final states of single-layer BiLSTM\n",
    "# Forward and backward cells both have 5 hidden units\n",
    "state_fw, state_bw = final_states\n",
    "\n",
    "# Concatenate along final axis\n",
    "final_c = tf.concat([state_fw.c, state_bw.c], -1)\n",
    "final_h = tf.concat([state_fw.h, state_bw.h], -1)\n",
    "\n",
    "combined_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "    final_c, final_h)\n",
    "print(combined_state)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Get c and h vectors for bidirectional LSTM final states\n",
    "def get_bi_state_parts(state_fw, state_bw):\n",
    "    bi_state_c = tf.concat([state_fw.c, state_bw.c], -1)\n",
    "    bi_state_h = tf.concat([state_fw.h, state_bw.h], -1)\n",
    "    return bi_state_c, bi_state_h\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Create multi-layer LSTM\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "\n",
    "    # Get embeddings for input/output sequences\n",
    "    def get_embeddings(self, sequences, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            cat_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "                'sequences',\n",
    "                self.extended_vocab_size)\n",
    "            embedding_column = tf.feature_column.embedding_column(\n",
    "                cat_column,\n",
    "                int(self.extended_vocab_size**0.25))\n",
    "            seq_dict = {'sequences': sequences}\n",
    "            embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "                seq_dict,\n",
    "                [embedding_column])\n",
    "            return embeddings, tf.cast(sequence_lengths, tf.int32)\n",
    "    \n",
    "    # Create the encoder for the model\n",
    "    def encoder(self, encoder_inputs, is_training):\n",
    "        input_embeddings, input_seq_lens = self.get_embeddings(encoder_inputs, 'encoder_emb')\n",
    "        cell_fw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        cell_bw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        enc_outputs, final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            input_embeddings,\n",
    "            sequence_length=input_seq_lens,\n",
    "            dtype=tf.float32)\n",
    "        states_fw, states_bw = final_states\n",
    "        combined_state = []\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            bi_state_c, bi_state_h = get_bi_state_parts(\n",
    "                states_fw[i], states_bw[i]\n",
    "            )\n",
    "            bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(bi_state_c, bi_state_h)\n",
    "            combined_state.append(bi_lstm_state)\n",
    "        final_state = tuple(combined_state)\n",
    "        return enc_outputs, input_seq_lens, final_state\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder\n",
    "\n",
    "#### Model architecture\n",
    "\n",
    "The decoder uses the final state of the encoder as its initial state, which gives it access to the information that the encoder extracted from the input sequence (which is crucial for good sequence-to-sequence modeling). \n",
    "\n",
    "![image](chapter2_encoder_decoder_outline.png)\n",
    "\n",
    "Above, the final state of the LSTM encoder for each layer is used as the starting state for the corresponding decoding layer. The encoder doesn't return outputs, and we only use the outputs of the decoder. \n",
    "\n",
    "The input tokens for the encoder represent the input sequence, while the input tokens for the decoder represent the ground truth tokens for the given input sequence. The decoder's output is equivalent to the output of a language model (i.e., each time step's output is based on the ground truth tokens from the previous time steps). \n",
    "\n",
    "#### Training vs. inference\n",
    "\n",
    "When we're training, we have access to both the input and output sequences for each training pair. However, when we're making predictions, we only have access to the input, so the decoder doesn't have access to ground truth tokens. We get around this by using the decoder's word predictions from previous time steps as our \"ground truth\" tokens (i.e., we use past predictions as our \"true\" values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "#### Using the encoder\n",
    "\n",
    "When we use an encoder-decoder model, the information that the decoder gets from the encoder is the final state of each layer. The final state essentially encapsulates/summarizes the encoder's extracted information from the input sequence. \n",
    "\n",
    "But, trying to encapsulate all the useful information from an input sequence is hard, especially if the input sequence is large and contains long-term dependencies. For example, decoders perform poorly on input sequences with long-term dependencies. \n",
    "\n",
    "For example, a text with the following input sequence is one that a regular encoder-decoder model would struggle to decode (e.g., for translation) because it's difficult to determine which part of the input is important:\n",
    "\n",
    "\"Sam grew up in Los Angeles. As a child, he dreamed of one day becoming an actor like Brad Pitt or Johnny Depp. Each day he would practice public speaking and impromptu skits near Venice Beach.\"\n",
    "\n",
    "To get around this, we can use the encoder's outputs as additional input for the decoder, which gives the decoder a lot more useful information about the input sequence. We do this by using **attention**\n",
    "\n",
    "#### How attention works\n",
    "\n",
    "Although we want to include the input sequence for the decoder, we don't necessarily want to use each input token equally, since we want the decoder to \"pay attention\" to certain outputs from the encoder. we let the decoder decide which encoder outputs are most useful for the decoder at the current decoding time step.\n",
    "\n",
    "Using the decoder's hidden state at the current time step, as well as the encoder outputs, attention will calculate something called a **context vector**, which encapsulates the most meaningful information from the input sequence for a current decoder time step, and it's used as additional input for the decoder when calculating the time step's output. \n",
    "\n",
    "![image](chapter2_attention.png)\n",
    "\n",
    "Attention uses trainable weights to calculate a context vector. It's like a mini neural network, which takes as input the decoder's current state and the encoder outputs, and uses its trainable weights to produce a context vector.\n",
    "\n",
    "#### Attention mechanisms\n",
    "\n",
    "The process for computing the context vector depends on the **attention mechanism** used. There are a few variations, but the popular ones in Tensorflow are `BahdanauAttention` and `LuongAttention`. The main difference between the two mechanisms is how they combine the encoder outputs and current time step hidden state when computing the context vector. The bahdanau mechanism uses an additive (concatenation-based) method, while the Luong mechanism uses a multiplicative method. \n",
    "\n",
    "Below is an example of implementing the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder representing the\n",
    "# individual lengths of each input sequence in the batch\n",
    "input_seq_lens = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "num_units = 8\n",
    "bahdanau = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units,\n",
    "    # combined encoder outputs (from previous chapter)\n",
    "    combined_enc_outputs,\n",
    "    memory_sequence_length=input_seq_lens)\n",
    "luong = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units,\n",
    "    # combined encoder outputs (from previous chapter)\n",
    "    combined_enc_outputs,\n",
    "    memory_sequence_length=input_seq_lens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow AttentionWrapper\n",
    "\n",
    "The implementation of attention requires a bit of linear algebra and advanced mathematics. So, TensorFlow gives an easy-to-use API for adding attention to an LSTM decoder cell via the AttentionWrapper function. Below is an example of using the AttentionWrapper.\n",
    "\n",
    "When using the decoder in TensorFlow, we pass the **attention value** at each decoder time step into the cell state at the next time step. The output of the fully-connected layer is used as the attention value. \n",
    "\n",
    "Using a fully-connected layer to create the attention value can benefit the model's performance, by using the decoder's outputs as additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Decoder LSTM cell\n",
    "dec_cell = tf.nn.rnn_cell.LSTMCell(8)\n",
    "dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    dec_cell,\n",
    "    luong, # LuongAttention object\n",
    "    attention_layer_size=8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Create multi-layer LSTM cells\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "\n",
    "    # Helper funtion to combine BiLSTM encoder outputs\n",
    "    def combine_enc_outputs(self, enc_outputs):\n",
    "        enc_outputs_fw, enc_outputs_bw = enc_outputs\n",
    "        return tf.concat([enc_outputs_fw, enc_outputs_bw], -1)\n",
    "\n",
    "    # Create the stacked LSTM cells for the decoder\n",
    "    def create_decoder_cell(self, enc_outputs, input_seq_lens, is_training):\n",
    "        num_decode_units = self.num_lstm_units * 2\n",
    "        dec_cell = self.stacked_lstm_cells(is_training, num_decode_units)\n",
    "        combined_enc_outputs = self.combine_enc_outputs(enc_outputs)\n",
    "        attention_mechanism = tf_s2s.LuongAttention(num_decode_units, combined_enc_units, memory_sequence_length = input_seq_lens)\n",
    "        dec_cell = tf_s2s.AttentionWrapper(dec_cell, attention_mechanism, attention_layer_size = num_decode_units)\n",
    "        return dec_cell\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Helper\n",
    "\n",
    "#### Decoding during training\n",
    "\n",
    "During training, we have both the input and output sequences of a training pair. So, we can use the output sequence's ground truth tokens as input for the decoder. \n",
    "\n",
    "Below is a helper function that we can use for decoding during training. \n",
    "\n",
    "The `TrainingHelper` object is initialized with the (embedding) ground truth sequences and the lengths of the ground truth sequences. Note that we use separate embedding models for the encoder input and the decoder input (i.e., ground truth tokens). This is because there are different word relationships in the input and output sequences for a seq2seq task, and sometimes the sequences can be completely different (e.g., machine translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder representing the\n",
    "# batch of (embedded) input sequences for the decoder\n",
    "# Shape: (batch_size, max_seq_len, embed_dim)\n",
    "decoder_embeddings = tf.placeholder(\n",
    "    tf.float32, shape=(None, None, 12)\n",
    ")\n",
    "\n",
    "# Placeholder representing the\n",
    "# individual lengths of each input sequence in the batch\n",
    "decoder_seq_lens = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_embeddings, decoder_seq_lens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    # Convert sequences to embeddings\n",
    "    def get_embeddings(self, sequences, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            cat_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "                'sequences',\n",
    "                self.extended_vocab_size)\n",
    "            embedding_column = tf.feature_column.embedding_column(\n",
    "                cat_column,\n",
    "                int(self.extended_vocab_size**0.25))\n",
    "            seq_dict = {'sequences': sequences}\n",
    "            embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "                seq_dict,\n",
    "                [embedding_column])\n",
    "            return embeddings, tf.cast(sequence_lengths, tf.int32)\n",
    "\n",
    "    # Create the helper for decoding\n",
    "    def create_decoder_helper(self, decoder_inputs, is_training, batch_size):\n",
    "        if is_training:\n",
    "            dec_embeddings, dec_seq_lens = self.get_embeddings(decoder_inputs, 'decoder_emb')\n",
    "            helper = tf_s2s.TrainingHelper(dec_embeddings, dec_seq_lens)\n",
    "            pass\n",
    "        else:\n",
    "            # IGNORE FOR NOW\n",
    "            pass\n",
    "        return helper, dec_seq_lens\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Object\n",
    "\n",
    "#### Creating the initial state\n",
    "\n",
    "The final state from the encoder is a tuple containing an `LSTMStateTuple` object for each layer of the BiLSTM. But, if we want to use this as the initial state for an attention-wrapped decoder, we need to convert it into an `AttentionWrapperState`.\n",
    "\n",
    "This process takes two steps: (1) use the \"zero_state\" function of the attention-wrapped decoder cell to create `AttentionWrapperState`, and (2) use the \"clone\" function to copy the encoder's final state into the blank `AttentionWrapperState`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 10\n",
    "# dec_cell is an attention-wrapped decoder LSTM cell\n",
    "zero_cell = dec_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# final_state is the final encoder state of a 2-layer BiLSTM\n",
    "initial_state = zero_cell.clone(cell_state=final_state)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The BasicDecoder object\n",
    "\n",
    "The decoder object that we use for decoding is the `BasicDecoder` object. The `BasicDecoder` constructor requires us to pass the decoder cell, helper object, and initial decoder state as required arguments, as well as a keyword argument \"output_layer\", which can be used to apply a fully-connected layer to the model's outputs (a shortcut for calculating the model's logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "num_units = 500 # extended vocab size\n",
    "projection_layer = tf.layers.Dense(num_units)\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    dec_cell, helper, initial_state,\n",
    "    output_layer=projection_layer)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "def create_basic_decoder(extended_vocab_size, batch_size, final_state, dec_cell, helper):\n",
    "    projection_layer = tf.layers.Dense(extended_vocab_size)\n",
    "    zero_cell = dec_cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = zero_cell.clone(cell_state = batch_size)\n",
    "    decoder = tf_s2s.BasicDecoder(dec_cell, helper, initial_state, output_layer = projection_layer)\n",
    "    return decoder\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "    \n",
    "    # Helper funtion to combine BiLSTM encoder outputs\n",
    "    def combine_enc_outputs(self, enc_outputs):\n",
    "        enc_outputs_fw, enc_outputs_bw = enc_outputs\n",
    "        return tf.concat([enc_outputs_fw, enc_outputs_bw], -1)\n",
    "    \n",
    "    # Create the stacked LSTM cells for the decoder\n",
    "    def create_decoder_cell(self, enc_outputs, input_seq_lens, is_training):\n",
    "        num_decode_units = self.num_lstm_units * 2\n",
    "        dec_cell = self.stacked_lstm_cells(is_training, num_decode_units)\n",
    "        combined_enc_outputs = self.combine_enc_outputs(enc_outputs)\n",
    "        attention_mechanism = tf_s2s.LuongAttention(\n",
    "            num_decode_units, combined_enc_outputs,\n",
    "            memory_sequence_length=input_seq_lens)\n",
    "        dec_cell = tf_s2s.AttentionWrapper(\n",
    "            dec_cell, attention_mechanism,\n",
    "            attention_layer_size=num_decode_units)\n",
    "        return dec_cell\n",
    "\n",
    "    # Create the helper for decoding\n",
    "    def create_decoder_helper(self, decoder_inputs, is_training, batch_size):\n",
    "        if is_training:\n",
    "            dec_embeddings, dec_seq_lens = self.get_embeddings(decoder_inputs, 'decoder_emb')\n",
    "            helper = tf_s2s.TrainingHelper(\n",
    "                dec_embeddings, dec_seq_lens)\n",
    "        else:\n",
    "            pass\n",
    "        return helper, dec_seq_lens\n",
    "\n",
    "    # Create the decoder for the model\n",
    "    def decoder(self, enc_outputs, input_seq_lens, final_state, batch_size,\n",
    "        decoder_inputs=None, maximum_iterations=None):\n",
    "        is_training = decoder_inputs is not None\n",
    "        dec_cell = self.create_decoder_cell(enc_outputs, input_seq_lens, is_training)\n",
    "        helper, dec_seq_lens = self.create_decoder_helper(decoder_inputs, is_training, batch_size)\n",
    "        decoder = create_basic_decoder(\n",
    "            self.extended_vocab_size, batch_size, final_state, dec_cell, helper)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Output\n",
    "\n",
    "#### Retrieving decoder outputs and returning the model's logits\n",
    "\n",
    "After creating the decoder object, we can perform the decoding using the `dynamic_decode` function. It takes in one required argument, the decoder object, and returns a tuple with three elements: (1) the decoder's output, `BasicDecoderOutput` object, (2) the decoder's final state (not used), and (3) the lengths of each of the decoder's output sequences (not used). \n",
    "\n",
    "#### Limiting the decoded length\n",
    "\n",
    "It's possible, especially in tasks like text summarization, that the decoder returns output sequences that are too long. We can manually limit the decoder output length with the `dynamic_decode` function's `maximum_iterations` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "extended_vocab_size = 500\n",
    "batch_size = 10\n",
    "# decoder is a BasicDecoder object\n",
    "outputs = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "\n",
    "decoder_output = outputs[0]\n",
    "logits = decoder_output.rnn_output\n",
    "decoder_final_state = outputs[1]\n",
    "decoded_sequence_lengths = outputs[2]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "def run_decoder(decoder, maximum_iterations, dec_seq_lens, is_training):\n",
    "    dec_outputs, _ = tf_s2s.dynamic_decode(decoder, maximum_terations = maximum_iterations)\n",
    "    # while training, return model's logits and the ground truth sequence lengths\n",
    "    if is_training:\n",
    "        return (dec_outputs, dec_seq_lens)\n",
    "    # while not training, return model's predictions\n",
    "    return dec_outputs.sample_id\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "    \n",
    "    # Helper funtion to combine BiLSTM encoder outputs\n",
    "    def combine_enc_outputs(self, enc_outputs):\n",
    "        enc_outputs_fw, enc_outputs_bw = enc_outputs\n",
    "        return tf.concat([enc_outputs_fw, enc_outputs_bw], -1)\n",
    "    \n",
    "    # Create the stacked LSTM cells for the decoder\n",
    "    def create_decoder_cell(self, enc_outputs, input_seq_lens, is_training):\n",
    "        num_decode_units = self.num_lstm_units * 2\n",
    "        dec_cell = self.stacked_lstm_cells(is_training, num_decode_units)\n",
    "        combined_enc_outputs = self.combine_enc_outputs(enc_outputs)\n",
    "        attention_mechanism = tf_s2s.LuongAttention(\n",
    "            num_decode_units, combined_enc_outputs,\n",
    "            memory_sequence_length=input_seq_lens)\n",
    "        dec_cell = tf_s2s.AttentionWrapper(\n",
    "            dec_cell, attention_mechanism,\n",
    "            attention_layer_size=num_decode_units)\n",
    "        return dec_cell\n",
    "\n",
    "    # Create the helper for decoding\n",
    "    def create_decoder_helper(self, decoder_inputs, is_training, batch_size):\n",
    "        if is_training:\n",
    "            dec_embeddings, dec_seq_lens = self.get_embeddings(decoder_inputs, 'decoder_emb')\n",
    "            helper = tf_s2s.TrainingHelper(\n",
    "                dec_embeddings, dec_seq_lens)\n",
    "        else:\n",
    "            pass\n",
    "        return helper, dec_seq_lens\n",
    "\n",
    "    # Create the decoder for the model\n",
    "    def decoder(self, enc_outputs, input_seq_lens, final_state, batch_size,\n",
    "        decoder_inputs=None, maximum_iterations=None):\n",
    "        is_training = decoder_inputs is not None\n",
    "        dec_cell = self.create_decoder_cell(enc_outputs, input_seq_lens, is_training)\n",
    "        helper, dec_seq_lens = self.create_decoder_helper(decoder_inputs, is_training, batch_size)\n",
    "        projection_layer = tf.layers.Dense(self.extended_vocab_size)\n",
    "        zero_cell = dec_cell.zero_state(batch_size, tf.float32)\n",
    "        initial_state = zero_cell.clone(cell_state=final_state)\n",
    "        decoder = tf_s2s.BasicDecoder(\n",
    "            dec_cell, helper, initial_state,\n",
    "            output_layer=projection_layer)\n",
    "        return run_decoder(decoder, maximum_iterations, dec_seq_lens, is_training)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Loss\n",
    "\n",
    "We can calculate the model's loss based on logits and sparse outputs\n",
    "\n",
    "#### Final token sequence\n",
    "\n",
    "The final token sequences are used when calculating the loss. If we view the decoder as a language model, the ground truth sequences act as the language model's input while the final token sequences act as the \"correct\" output for the language model. \n",
    "\n",
    "So, we compare the \"true\" sequences with the final token sequences.\n",
    "\n",
    "We use a sparse softmax cross entropy loss based on the logits and final token sequences, and then zero-out the time steps that correspond to padding. \n",
    "\n",
    "#### Sequence mask\n",
    "\n",
    "We can use `tf.sequence_mask`, which lets us create binary sequences, known as a sequence mask (which we did before, in order to zero-out any padding that we implemented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example sequence lengths\n",
    "seq_lens = tf.constant([3, 4, 2])\n",
    "binary_sequences = tf.sequence_mask(seq_lens)\n",
    "int_sequences = tf.sequence_mask(seq_lens, dtype=tf.int32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    binary_array = sess.run(binary_sequences)\n",
    "    int_array = sess.run(int_sequences)\n",
    "\n",
    "print(repr(binary_array))\n",
    "print(repr(int_array))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    # Calculate the model loss\n",
    "    def calculate_loss(self, logits, dec_seq_lens, decoder_outputs, batch_size):\n",
    "        binary_sequences = tf.sequence_mask(dec_seq_lens, dtype=tf.float32)\n",
    "        batch_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=decoder_outputs, logits=logits)\n",
    "        unpadded_loss = batch_loss * binary_sequences\n",
    "        per_seq_loss = tf.reduce_sum(unpadded_loss) / batch_size\n",
    "        return per_seq_loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Decodiing\n",
    "\n",
    "#### Decoding without ground truth\n",
    "\n",
    "While we train, we have access to ground truth tokens (i.e., what the actual output was), but for inference / predictions (e.g., using Google Translate to generate translations in real-time), we only have the input sequence. So, the input for the decoder at each time step is just the decoder's output token from the previous time step. \n",
    "\n",
    "![image](chapter2_decoder_inference.png)\n",
    "\n",
    "We can switch from training to inference mode just by changing the \"TrainingHelper\" object to an inference helper. The most commonly used inference helper object is `GreedyEmbeddingHelper`.\n",
    "\n",
    "#### Greedy decoding\n",
    "\n",
    "`GreedyEmbeddingHelper` utilizes a greedy decoding method and incorporates an embedding matrix. The output at each decoder time step (which is also the input into the next time step) is the highest probability vocabulary word at the time step. The output is converted into a word embedding prior to being used as input for the next time step. Below is an example of a `GreedyEmbeddingHelper` object, with a vocabulary size of 598 and a batch size of 8. We initialize it with an embedding matrix, a \"batch_size\" length tensor of SOS tokens, and the EOS token. The embedded SOS token is always the initial decoder input token, and isused to give the decoder a starting point (and marks the beginning of the decoded output). The EOS token marks the end of a decoded sequences, and when the decoder returns the EOS token at some time step, the decoding process is terminated.\n",
    "\n",
    "#### Embedding variable scope\n",
    "\n",
    "When using feature columns to create embeddings, we don't have direct access to the embedding matrix variable, so we need to create an embedding matrix variable with the same name as the one used in training. Below is an example of specifying variable scope in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### CREATING THE GREEDY EMBEDDING HELPER\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 598\n",
    "sos = vocab_size\n",
    "eos = vocab_size + 1\n",
    "extended_vocab_size = vocab_size + 2\n",
    "\n",
    "# Placeholder representing the embedding matrix for the vocab\n",
    "# Embedding dim is 10\n",
    "embedding_matrix = tf.placeholder(\n",
    "    tf.float32, shape=(600, 10)\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "start_tokens = tf.tile([sos], [batch_size])\n",
    "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding_matrix, start_tokens, eos)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "### CHANGING SCOPE\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.variable_scope('scope1'):\n",
    "    v1 = tf.get_variable('var', shape=(2, 2))\n",
    "\n",
    "with tf.variable_scope('scope2'):\n",
    "    v2 = tf.get_variable('var', shape=(2, 2))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "### CREATING GREEDY EMBEDDING HELPER, V2\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    # Create the helper for decoding\n",
    "    def create_decoder_helper(self, decoder_inputs, is_training, batch_size):\n",
    "        if is_training:\n",
    "            # ALREADY COMPLETED IN TRAINING HELPER CHAPTER\n",
    "            pass\n",
    "        else:\n",
    "            DEC_EMB_SCOPE = 'decoder_emb/sequence_input_layer/sequences_embedding'\n",
    "            with tf.variable_scope(DEC_EMB_SCOPE):\n",
    "                embedding_weights = tf.get_variable(\n",
    "                    'embedding_weights',\n",
    "                    shape=(self.extended_vocab_size, int(self.extended_vocab_size**0.25)))\n",
    "            start_tokens = tf.tile([self.vocab_size], [batch_size])\n",
    "            end_token = self.vocab_size + 1\n",
    "            helper = tf_s2s.GreedyEmbeddingHelper(embedding_weights, start_tokens, end_token)\n",
    "            dec_seq_lens = None\n",
    "        return helper, dec_seq_lens\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement\n",
    "\n",
    "How can we improve the performance of an encoder-decoder model?\n",
    "\n",
    "#### Training strategies\n",
    "\n",
    "Good encoder-decoder models tend to have a large number of weight parameters, since they consist of large LSTM/BiLSTM layers. Because of this, it can take time to train an encoder-decoder model to convergence.\n",
    "\n",
    "To speed up training, we can use a larger batch size during the initial stages of training, and then reduce the batch size once the model begins to show considerable improvement in reducing the loss function. A larger batch size allows the model to get through more data, and as long as we reduce the batch size early enough, the model won't miss its convergence point. \n",
    "\n",
    "We can also speed up training by starting off with a larger learning rate, and then gradually decreasing the learning rate as the model trains. \n",
    "\n",
    "#### Domain-specific strategies\n",
    "\n",
    "The encoder-decoder model is useful for any seq2seq task, whcih involves domains such as machine translation, text summarization, and chatbots. Each of these domains has their own special features, which can inspire how we tweak our encoder-decoder models.\n",
    "\n",
    "For example, in text summarization, it often helps to truncate the input text to a maximum lengt, since input can be long and because much of the important information can be extracted in less text (so, adding more text can create extra noise). \n",
    "\n",
    "Another example is with chatbots. For example, we'd want to train a customer service chatbot on data that contains dialogue specific to customer service, and we also want to prune out words, such as profanity, that we don't want in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Datasets \n",
    "\n",
    "\n",
    "To practice Seq2Seq and encoder-decoder models, the following are good datasets to use:\n",
    "\n",
    "Machine translation: http://www.statmt.org/wmt18/translation-task.html\n",
    "\n",
    "Chatbots:\n",
    "1. http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/\n",
    "2. https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "Text summarization:\n",
    "1. https://github.com/abisee/cnn-dailymail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
