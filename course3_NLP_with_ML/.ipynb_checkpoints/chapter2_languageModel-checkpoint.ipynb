{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this section, we'll use an LSTM (long short-term memory) network for language modeling. \n",
    "\n",
    "#### Language modeling\n",
    "\n",
    "A language model can tell us the likelihood of each word in a sentence or passage, based on the words that came before it. We can then determine how likely a sentence or text passage is by aggregating its individual word probabilities.\n",
    "\n",
    "#### Language model tasks\n",
    "\n",
    "Languagemodels are useful for both text classification and generation. In classification, we can use the language model probabilities to separate text into different categories.\n",
    "\n",
    "For example, if we trained a language model on spam email subject titles, we could take a title and train the model to identify it. \n",
    "\n",
    "We can also use a language model to generate text based off an incomplete input sentence (e.g., autocomplete). The model gives suggestions to complete the sentence, based off using words with the highest probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "\n",
    "We can use language models to calculate word probabilities.\n",
    "\n",
    "#### Word probabilities\n",
    "\n",
    "The probability for each word is conditioned on the words that appear before it in the sequence. Because of this, the task of calculating a word probability based on the previous words in a sequence is essentially multiclass classification. \n",
    "\n",
    "Since each word in a sequence must come from the text corpus' vocabulary, we consider each vocabulary word as a separate class. We can use the previous sequence words as input and the word of interest as the word that we are predicting, in order to calculate probabilities for each vocabulary class.\n",
    "\n",
    "![title](chapter2_wordProbs.png)\n",
    "\n",
    "#### Inputs & targets\n",
    "\n",
    "The goal behind training a language model is to predict each word in a sequence based on the words that come before it. \n",
    "\n",
    "But, instead of making training pairs where the target is a single word, we can make training pairs where the input and target sequences are the same length. \n",
    "\n",
    "![title](chapter2_example_training_words.png)\n",
    "\n",
    "The language model attempts to predict each word in the target sequence based on its corresponding prefix in the input sequence. In the example, the input prefix-target word pairs are: \n",
    "\n",
    "![title](chapter2_language_pairs.png)\n",
    "\n",
    "So, we can predict say, the third word in the text by using the first two words as the \"prefix\" and the third word as the target. \n",
    "\n",
    "#### Maximum length\n",
    "\n",
    "In this setup, we see that words later on in the string will have a longer prefix than words earlier in the string. We can put a limit on the length of the training sequences. \n",
    "\n",
    "Using a fixed max sequence length can increase training speed and help the model avoid overfitting on uncommon text dependencies (which might show up more often in long, run-on sentences that are likely just people rambling).\n",
    "\n",
    "![title](chapter2_language_fixed_length.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "def truncate_sequences(sequence, max_length):\n",
    "    # CODE HERE\n",
    "    input_sequence = sequence[:max_length-1]\n",
    "    target_sequence = sequence[1:max_length]\n",
    "    return (input_sequence, target_sequence)\n",
    "\n",
    "# LSTM Language Model\n",
    "class LanguageModel(object):\n",
    "    # Model Initialization\n",
    "    def __init__(self, vocab_size, max_length, num_lstm_units, num_lstm_layers):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    def get_input_target_sequence(self, sequence):\n",
    "        seq_len = len(sequence)\n",
    "        if seq_len >= self.max_length:\n",
    "            input_sequence, target_sequence = truncate_sequences(\n",
    "                sequence, self.max_length\n",
    "            )\n",
    "        else:\n",
    "            # Next chapter\n",
    "            input_sequence, target_sequence = pad_sequences(\n",
    "                sequence, self.max_length\n",
    "            )\n",
    "        return input_sequence, target_sequence\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "We can use padding for our tokenized sequences\n",
    "\n",
    "#### Varied length sequences\n",
    "\n",
    "For most neural networks, the input data always has a fixed length. This is the case because most neural networks are ***feed-forward***, which means that they use multiple layers of ****fixed**** sizes to compute the network's output. \n",
    "\n",
    "But, since text data involves different-sized text sequences (E.g., sentences, passages, paragraphs, etc.), the language model must be flexible enough to handle input data of different lengths. \n",
    "\n",
    "As a result, we use a recurrent neural network (RNN) for the language model. \n",
    "\n",
    "#### Padded sequences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
