{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "Here, we're building a sequence-to-sequence (seq2seq) model, which is used for tasks that involve reading in a sequence of text and generating an output text sequence based on the input. \n",
    "\n",
    "#### Sequence to sequence\n",
    "\n",
    "The sequence to sequence (seq2seq) framework encompasses any task that involves taking in some text and returning some generated text. Some examples of this include chatbots, text summarization, and machine translation. \n",
    "\n",
    "In the past, these seq2seq tasks were often performed using Bayesian statistics. But, we've been able to apply deep learning to these tasks.\n",
    "\n",
    "In particular, there's an extremely powerful model called the \"encoder-decoder\", which is specifically designed for seq2seq applications.\n",
    "\n",
    "The encoder-decoder is named for its two parts: the encoder and the decoder. Both the encoder and decoder are langauge models. \n",
    "\n",
    "First, an input sequence is fed to the encoder. The output from the last layer of the encoder becomes the input for the first layer of the decoder. The decoder transforms that input back into a text sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Seq2Seq (from paper)\n",
    "\n",
    "See this paper: (https://arxiv.org/pdf/1409.3215v3.pdf)\n",
    "See this tutorial: (https://www.youtube.com/watch?v=ElmBrKyMXxs)\n",
    "\n",
    "When you train a Seq2Seq model, you have an encoder network and a decoder network. You split a sentence such as \"I know that all dogs are really good pets\" into something like \"I know that all dogs\" and \"are really good pets\".\n",
    "\n",
    "In the encoding portion, you run \"I know that all dogs\" and get the output and a hidden state. You feed these (not sure about output, but at the very least, you feed in the hidden state) into a decoder, which will try to recreate \"are really good pets\". \n",
    "\n",
    "As you do this over and over, the decoder output should better approximate the true output sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "#### Training task\n",
    "\n",
    "For a seq2seq model, we use training pairs that contain an input sequence and an output sequence (and the goal is to predict the output sequence).\n",
    "\n",
    "During training, we perform two tasks:\n",
    "1. **Input Task**: Extract useful information from the input sequence\n",
    "2. **Output Task**: Calculate word probabilities at each output time step, using information from the input sequence and **previous** words in the output sequence.\n",
    "\n",
    "We can leave the input sequence as is. We process the output sequence into two separate sequences: the ground truth sequence and the final token sequence.\n",
    "\n",
    "#### Processing the output\n",
    "\n",
    "The ground truth sequence for a seq2seq model is equal to the input sequence for a language model - it represents sequence prefixes that we use to calculate word probabilities at each time step of the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
