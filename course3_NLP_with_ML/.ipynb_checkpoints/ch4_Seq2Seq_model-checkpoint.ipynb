{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "Here, we're building a sequence-to-sequence (seq2seq) model, which is used for tasks that involve reading in a sequence of text and generating an output text sequence based on the input. \n",
    "\n",
    "#### Sequence to sequence\n",
    "\n",
    "The sequence to sequence (seq2seq) framework encompasses any task that involves taking in some text and returning some generated text. Some examples of this include chatbots, text summarization, and machine translation. \n",
    "\n",
    "In the past, these seq2seq tasks were often performed using Bayesian statistics. But, we've been able to apply deep learning to these tasks.\n",
    "\n",
    "In particular, there's an extremely powerful model called the \"encoder-decoder\", which is specifically designed for seq2seq applications.\n",
    "\n",
    "The encoder-decoder is named for its two parts: the encoder and the decoder. Both the encoder and decoder are langauge models. \n",
    "\n",
    "First, an input sequence is fed to the encoder. The output from the last layer of the encoder becomes the input for the first layer of the decoder. The decoder transforms that input back into a text sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Seq2Seq (from paper)\n",
    "\n",
    "See this paper: (https://arxiv.org/pdf/1409.3215v3.pdf)\n",
    "See this tutorial: (https://www.youtube.com/watch?v=ElmBrKyMXxs)\n",
    "\n",
    "When you train a Seq2Seq model, you have an encoder network and a decoder network. You split a sentence such as \"I know that all dogs are really good pets\" into something like \"I know that all dogs\" and \"are really good pets\".\n",
    "\n",
    "In the encoding portion, you run \"I know that all dogs\" and get the output and a hidden state. You feed these (not sure about output, but at the very least, you feed in the hidden state) into a decoder, which will try to recreate \"are really good pets\". \n",
    "\n",
    "As you do this over and over, the decoder output should better approximate the true output sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "#### Training task\n",
    "\n",
    "For a seq2seq model, we use training pairs that contain an input sequence and an output sequence (and the goal is to predict the output sequence).\n",
    "\n",
    "During training, we perform two tasks:\n",
    "1. **Input Task**: Extract useful information from the input sequence\n",
    "2. **Output Task**: Calculate word probabilities at each output time step, using information from the input sequence and **previous** words in the output sequence.\n",
    "\n",
    "We can leave the input sequence as is. We process the output sequence into two separate sequences: the ground truth sequence and the final token sequence.\n",
    "\n",
    "#### Processing the output\n",
    "\n",
    "The ground truth sequence for a seq2seq model is equal to the input sequence for a language model - it represents sequence prefixes that we use to calculate word probabilities at each time step of the output. \n",
    "\n",
    "#### SOS and EOS tokens\n",
    "\n",
    "For seq2seq models, we need to have start-of-sequence (SOS) and end-of-sequence (EOS) tokens, which mark the start and end of a tokenized text sequence. \n",
    "\n",
    "Example. ['SOS', 'he', 'eats', 'bread', 'EOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    # Create a sequence training tuple from input/output sequences\n",
    "    def make_training_tuple(self, input_sequence, output_sequence):\n",
    "        truncate_front = output_sequence[1:]\n",
    "        truncate_back = output_sequence[:-1]\n",
    "        sos_token = [self.vocab_size]\n",
    "        eos_token = [self.vocab_size + 1]\n",
    "        # create input, output sequences\n",
    "        input_sequence = [sos_token] + input_sequence + [eos_token]\n",
    "        ground_truth = truncate_back + [sos_token]\n",
    "        final_sequence = truncate_front + [eos_token]\n",
    "        return (input_sequence, ground_truth, final_sequence)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final States\n",
    "\n",
    "Here, let's compare the final state output of an LSTM and BiLSTM\n",
    "\n",
    "#### The encoder\n",
    "\n",
    "The encoder is used to extract useful information, and is typically an LSTM or BiLSTM\n",
    "\n",
    "#### LSTM final state\n",
    "\n",
    "We pass the final state of the encoder into the decoder. For an LSTM in Tensorflow, the final state is represented by the LSTMStateTuple object, which contains two important properties: (1) the hidden state (c) and (2) the state output (h). The hidden state represents the internal cell state (\"memory\") of the LSTM cell. \n",
    "\n",
    "The two properties are represented by tensors with shape = (batch_size, hidden_units)\n",
    "\n",
    "#### Multi-layer final states\n",
    "\n",
    "For a multi-layer LSTM, the final state output of `dynamic_rnn` is a tuple containing the final state for each layer.\n",
    "\n",
    "#### BiLSTM final state\n",
    "\n",
    "The final state of a BiLSTM is similar to that of the LSTM, except the output is a tuple of two LSTMStateTuple objects (one for the forward LSTM and one for the backward LSTM)\n",
    "\n",
    "#### Combining forward and backward, for BiLSTM\n",
    "\n",
    "Since the decoder portion utilizes only a regular LSTM (which works in the forward direction), we need to combine the forward and backward LSTMs, through concatenating the hidden state and state output of both the forward and backward states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Get c and h vectors for bidirectional LSTM final states\n",
    "def get_bi_state_parts(state_fw, state_bw):\n",
    "    bi_state_c = tf.concat([state_fw.c, state_bw.c],-1)\n",
    "    bi_state_h = tf.concat([state_fw.h, state_bw.h], -1)\n",
    "    return (bi_state_c, bi_state_h)\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Create multi-layer LSTM\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "\n",
    "    # Get embeddings for input/output sequences\n",
    "    def get_embeddings(self, sequences, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            cat_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "                'sequences',\n",
    "                self.extended_vocab_size)\n",
    "            embedding_column = tf.feature_column.embedding_column(\n",
    "                cat_column,\n",
    "                int(self.extended_vocab_size**0.25))\n",
    "            seq_dict = {'sequences': sequences}\n",
    "            embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "                seq_dict,\n",
    "                [embedding_column])\n",
    "            return embeddings, tf.cast(sequence_lengths, tf.int32)\n",
    "    \n",
    "    # Create the encoder for the model\n",
    "    def encoder(self, encoder_inputs, is_training):\n",
    "        input_embeddings, input_seq_lens = self.get_embeddings(encoder_inputs, 'encoder_emb')\n",
    "        cell_fw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        cell_bw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        enc_outputs, final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            input_embeddings,\n",
    "            sequence_length=input_seq_lens,\n",
    "            dtype=tf.float32)\n",
    "        states_fw, states_bw = final_states\n",
    "        combined_state = []\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            bi_state_c, bi_state_h = get_bi_state_parts(\n",
    "                states_fw[i], states_bw[i]\n",
    "            )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined State\n",
    "\n",
    "We need to combine the final states for a BiLSTM into usable initial states\n",
    "\n",
    "#### LSTMStateTuple initialization\n",
    "\n",
    "We can initialize an LSTMStateTuple object with a hidden state (c) and state output (h). \n",
    "\n",
    "Below is an example of combining the BiLSTM forward and backward states into a single LSTMStateTuple object, which can be passed into the decoder. \n",
    "\n",
    "For BiLSTM encoders with multiple layers, we combine the states for each layer to create a tuple of `LSTMStateTuple` objects, where the element at index \"i\" of the tuple is the \"ith\" layer's combined final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Final states of single-layer BiLSTM\n",
    "# Forward and backward cells both have 5 hidden units\n",
    "state_fw, state_bw = final_states\n",
    "\n",
    "# Concatenate along final axis\n",
    "final_c = tf.concat([state_fw.c, state_bw.c], -1)\n",
    "final_h = tf.concat([state_fw.h, state_bw.h], -1)\n",
    "\n",
    "combined_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "    final_c, final_h)\n",
    "print(combined_state)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Get c and h vectors for bidirectional LSTM final states\n",
    "def get_bi_state_parts(state_fw, state_bw):\n",
    "    bi_state_c = tf.concat([state_fw.c, state_bw.c], -1)\n",
    "    bi_state_h = tf.concat([state_fw.h, state_bw.h], -1)\n",
    "    return bi_state_c, bi_state_h\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "\n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Create multi-layer LSTM\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "\n",
    "    # Get embeddings for input/output sequences\n",
    "    def get_embeddings(self, sequences, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            cat_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "                'sequences',\n",
    "                self.extended_vocab_size)\n",
    "            embedding_column = tf.feature_column.embedding_column(\n",
    "                cat_column,\n",
    "                int(self.extended_vocab_size**0.25))\n",
    "            seq_dict = {'sequences': sequences}\n",
    "            embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "                seq_dict,\n",
    "                [embedding_column])\n",
    "            return embeddings, tf.cast(sequence_lengths, tf.int32)\n",
    "    \n",
    "    # Create the encoder for the model\n",
    "    def encoder(self, encoder_inputs, is_training):\n",
    "        input_embeddings, input_seq_lens = self.get_embeddings(encoder_inputs, 'encoder_emb')\n",
    "        cell_fw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        cell_bw = self.stacked_lstm_cells(is_training, self.num_lstm_units)\n",
    "        enc_outputs, final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            input_embeddings,\n",
    "            sequence_length=input_seq_lens,\n",
    "            dtype=tf.float32)\n",
    "        states_fw, states_bw = final_states\n",
    "        combined_state = []\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            bi_state_c, bi_state_h = get_bi_state_parts(\n",
    "                states_fw[i], states_bw[i]\n",
    "            )\n",
    "            bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(bi_state_c, bi_state_h)\n",
    "            combined_state.append(bi_lstm_state)\n",
    "        final_state = tuple(combined_state)\n",
    "        return enc_outputs, input_seq_lens, final_state\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder\n",
    "\n",
    "#### Model architecture\n",
    "\n",
    "The decoder uses the final state of the encoder as its initial state, which gives it access to the information that the encoder extracted from the input sequence (which is crucial for good sequence-to-sequence modeling). \n",
    "\n",
    "![image](chapter2_encoder_decoder_outline.png)\n",
    "\n",
    "Above, the final state of the LSTM encoder for each layer is used as the starting state for the corresponding decoding layer. The encoder doesn't return outputs, and we only use the outputs of the decoder. \n",
    "\n",
    "The input tokens for the encoder represent the input sequence, while the input tokens for the decoder represent the ground truth tokens for the given input sequence. The decoder's output is equivalent to the output of a language model (i.e., each time step's output is based on the ground truth tokens from the previous time steps). \n",
    "\n",
    "#### Training vs. inference\n",
    "\n",
    "When we're training, we have access to both the input and output sequences for each training pair. However, when we're making predictions, we only have access to the input, so the decoder doesn't have access to ground truth tokens. We get around this by using the decoder's word predictions from previous time steps as our \"ground truth\" tokens (i.e., we use past predictions as our \"true\" values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "#### Using the encoder\n",
    "\n",
    "When we use an encoder-decoder model, the information that the decoder gets from the encoder is the final state of each layer. The final state essentially encapsulates/summarizes the encoder's extracted information from the input sequence. \n",
    "\n",
    "But, trying to encapsulate all the useful information from an input sequence is hard, especially if the input sequence is large and contains long-term dependencies. For example, decoders perform poorly on input sequences with long-term dependencies. \n",
    "\n",
    "For example, a text with the following input sequence is one that a regular encoder-decoder model would struggle to decode (e.g., for translation) because it's difficult to determine which part of the input is important:\n",
    "\n",
    "\"Sam grew up in Los Angeles. As a child, he dreamed of one day becoming an actor like Brad Pitt or Johnny Depp. Each day he would practice public speaking and impromptu skits near Venice Beach.\"\n",
    "\n",
    "To get around this, we can use the encoder's outputs as additional input for the decoder, which gives the decoder a lot more useful information about the input sequence. We do this by using **attention**\n",
    "\n",
    "#### How attention works\n",
    "\n",
    "Although we want to include the input sequence for the decoder, we don't necessarily want to use each input token equally, since we want the decoder to \"pay attention\" to certain outputs from the encoder. we let the decoder decide which encoder outputs are most useful for the decoder at the current decoding time step.\n",
    "\n",
    "Using the decoder's hidden state at the current time step, as well as the encoder outputs, attention will calculate something called a **context vector**, which encapsulates the most meaningful information from the input sequence for a current decoder time step, and it's used as additional input for the decoder when calculating the time step's output. \n",
    "\n",
    "![image](chapter2_attention.png)\n",
    "\n",
    "Attention uses trainable weights to calculate a context vector. It's like a mini neural network, which takes as input the decoder's current state and the encoder outputs, and uses its trainable weights to produce a context vector.\n",
    "\n",
    "#### Attention mechanisms\n",
    "\n",
    "The process for computing the context vector depends on the **attention mechanism** used. There are a few variations, but the popular ones in Tensorflow are `BahdanauAttention` and `LuongAttention`. The main difference between the two mechanisms is how they combine the encoder outputs and current time step hidden state when computing the context vector. The bahdanau mechanism uses an additive (concatenation-based) method, while the Luong mechanism uses a multiplicative method. \n",
    "\n",
    "Below is an example of implementing the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder representing the\n",
    "# individual lengths of each input sequence in the batch\n",
    "input_seq_lens = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "num_units = 8\n",
    "bahdanau = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units,\n",
    "    # combined encoder outputs (from previous chapter)\n",
    "    combined_enc_outputs,\n",
    "    memory_sequence_length=input_seq_lens)\n",
    "luong = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units,\n",
    "    # combined encoder outputs (from previous chapter)\n",
    "    combined_enc_outputs,\n",
    "    memory_sequence_length=input_seq_lens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow AttentionWrapper\n",
    "\n",
    "The implementation of attention requires a bit of linear algebra and advanced mathematics. So, TensorFlow gives an easy-to-use API for adding attention to an LSTM decoder cell via the AttentionWrapper function. Below is an example of using the AttentionWrapper.\n",
    "\n",
    "When using the decoder in TensorFlow, we pass the **attention value** at each decoder time step into the cell state at the next time step. The output of the fully-connected layer is used as the attention value. \n",
    "\n",
    "Using a fully-connected layer to create the attention value can benefit the model's performance, by using the decoder's outputs as additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Decoder LSTM cell\n",
    "dec_cell = tf.nn.rnn_cell.LSTMCell(8)\n",
    "dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    dec_cell,\n",
    "    luong, # LuongAttention object\n",
    "    attention_layer_size=8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    def make_lstm_cell(self, dropout_keep_prob, num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "    # Create multi-layer LSTM cells\n",
    "    def stacked_lstm_cells(self, is_training, num_units):\n",
    "        dropout_keep_prob = 0.5 if is_training else 1.0\n",
    "        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        return cell\n",
    "\n",
    "    # Helper funtion to combine BiLSTM encoder outputs\n",
    "    def combine_enc_outputs(self, enc_outputs):\n",
    "        enc_outputs_fw, enc_outputs_bw = enc_outputs\n",
    "        return tf.concat([enc_outputs_fw, enc_outputs_bw], -1)\n",
    "\n",
    "    # Create the stacked LSTM cells for the decoder\n",
    "    def create_decoder_cell(self, enc_outputs, input_seq_lens, is_training):\n",
    "        num_decode_units = self.num_lstm_units * 2\n",
    "        dec_cell = self.stacked_lstm_cells(is_training, num_decode_units)\n",
    "        combined_enc_outputs = self.combine_enc_outputs(enc_outputs)\n",
    "        attention_mechanism = tf_s2s.LuongAttention(num_decode_units, combined_enc_units, memory_sequence_length = input_seq_lens)\n",
    "        dec_cell = tf_s2s.AttentionWrapper(dec_cell, attention_mechanism, attention_layer_size = num_decode_units)\n",
    "        return dec_cell\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Helper\n",
    "\n",
    "#### Decoding during training\n",
    "\n",
    "During training, we have both the input and output sequences of a training pair. So, we can use the output sequence's ground truth tokens as input for the decoder. \n",
    "\n",
    "Below is a helper function that we can use for decoding during training. \n",
    "\n",
    "The `TrainingHelper` object is initialized with the (embedding) ground truth sequences and the lengths of the ground truth sequences. Note that we use separate embedding models for the encoder input and the decoder input (i.e., ground truth tokens). This is because there are different word relationships in the input and output sequences for a seq2seq task, and sometimes the sequences can be completely different (e.g., machine translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder representing the\n",
    "# batch of (embedded) input sequences for the decoder\n",
    "# Shape: (batch_size, max_seq_len, embed_dim)\n",
    "decoder_embeddings = tf.placeholder(\n",
    "    tf.float32, shape=(None, None, 12)\n",
    ")\n",
    "\n",
    "# Placeholder representing the\n",
    "# individual lengths of each input sequence in the batch\n",
    "decoder_seq_lens = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_embeddings, decoder_seq_lens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_fc = tf.contrib.feature_column\n",
    "tf_s2s = tf.contrib.seq2seq\n",
    "\n",
    "# Seq2seq model\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Extended vocabulary includes start, stop token\n",
    "        self.extended_vocab_size = vocab_size + 2\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_lstm_units = num_lstm_units\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size)\n",
    "    \n",
    "    # Convert sequences to embeddings\n",
    "    def get_embeddings(self, sequences, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            cat_column = tf_fc.sequence_categorical_column_with_identity(\n",
    "                'sequences',\n",
    "                self.extended_vocab_size)\n",
    "            embedding_column = tf.feature_column.embedding_column(\n",
    "                cat_column,\n",
    "                int(self.extended_vocab_size**0.25))\n",
    "            seq_dict = {'sequences': sequences}\n",
    "            embeddings, sequence_lengths = tf_fc.sequence_input_layer(\n",
    "                seq_dict,\n",
    "                [embedding_column])\n",
    "            return embeddings, tf.cast(sequence_lengths, tf.int32)\n",
    "\n",
    "    # Create the helper for decoding\n",
    "    def create_decoder_helper(self, decoder_inputs, is_training, batch_size):\n",
    "        if is_training:\n",
    "            dec_embeddings, dec_seq_lens = self.get_embeddings(decoder_inputs, 'decoder_emb')\n",
    "            helper = tf_s2s.TrainingHelper(dec_embeddings, dec_seq_lens)\n",
    "            pass\n",
    "        else:\n",
    "            # IGNORE FOR NOW\n",
    "            pass\n",
    "        return helper, dec_seq_lens\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
