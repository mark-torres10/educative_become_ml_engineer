{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the XGBoost library, which makes use of gradient boosted trees for classification and regression. \n",
    "\n",
    "The problem with regular trees is often that they lack complexity and sophistication. They're often unable to capture the intricacies of large datasets. We could continuously increase the maximum depth of a decision tree to fit larger datasets, but decision trees with many nodes tend to overfit the data.\n",
    "\n",
    "Instead, we make use of gradient boosting to combine many decision trees into a single model for classification or regression. Gradient boosting starts with a single decision tree, then adds more decision trees to the overal model that correct the existing model's errors on the training dataset. Each tree added is \"built off the errors of the others\", so to speak. \n",
    "\n",
    "(can't install xgboost on my system? Wonder why... some error keeps popping up and I'm not sure how to fix it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Basics\n",
    "\n",
    "The basic data structure in XGBoost is the DMatrix, which is just a data matrix. The DMatrix can be used to train a Booster() object, which represents the gradient boosted decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = np.array([\\n  [1.2, 3.3, 1.4],\\n  [5.1, 2.2, 6.6]])\\n\\nimport xgboost as xgb\\ndmat1 = xgb.DMatrix(data)\\n\\nlabels = np.array([0, 1])\\ndmat2 = xgb.DMatrix(data, label=labels)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = np.array([\n",
    "  [1.2, 3.3, 1.4],\n",
    "  [5.1, 2.2, 6.6]])\n",
    "\n",
    "import xgboost as xgb\n",
    "dmat1 = xgb.DMatrix(data)\n",
    "\n",
    "labels = np.array([0, 1])\n",
    "dmat2 = xgb.DMatrix(data, label=labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predefined data and labels\n",
    "print('Data shape: {}'.format(data.shape))\n",
    "print('Labels shape: {}'.format(labels.shape))\n",
    "dtrain = xgb.DMatrix(data, label=labels)\n",
    "\n",
    "# training parameters\n",
    "params = {\n",
    "  'max_depth': 0,\n",
    "  'objective': 'binary:logistic'\n",
    "}\n",
    "print('Start training')\n",
    "bst = xgb.train(params, dtrain)  # booster\n",
    "print('Finish training')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we set the 'max_depth' parameter to 0 (which means no limit on the tree depths, equivalent to None in scikit-learn). We also set the 'objective' parameter (the objective function) to binary classification via logistic regression. For the remaining available parameters, we used their default settings (so we didn't include them in params).\n",
    "\n",
    "After we train a booster, we can use it to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predefined evaluation data and labels\n",
    "print('Data shape: {}'.format(eval_data.shape))\n",
    "print('Labels shape: {}'.format(eval_labels.shape))\n",
    "deval = xgb.DMatrix(eval_data, label=eval_labels)\n",
    "\n",
    "# Trained bst from previous code\n",
    "print(bst.eval(deval))  # evaluation\n",
    "\n",
    "# new_data contains 2 new data observations\n",
    "dpred = xgb.DMatrix(new_data)\n",
    "# predictions represents probabilities\n",
    "predictions = bst.predict(dpred)\n",
    "print('{}\\n'.format(predictions))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "\n",
    "Data shape: (119, 30)\n",
    "Labels shape: (119,)\n",
    "[0]\teval-error:0.226891\n",
    "[0.6236573 0.6236573]\n",
    "\"\"\"\n",
    "\n",
    "# for binary classification, the default metric is eval-error, which is the classification error. \n",
    "# Note that the model's predictions (from the predict function) are probabilities, rather than class labels.  \n",
    "# The actual label classifications are just the rounded probabilities. In the example above, \n",
    "# the Booster predicts classes of 0 and 1, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cross-validate XGBoost using xgb.cv. The keyword 'num_boost_round' specifies the number of boosting iterations, where each boosting iteration will try to improve the model through boosting. \n",
    "\n",
    "You can specify n_fold (default = 3) and num_boost_round (default = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# predefined data and labels\\ndtrain = xgb.DMatrix(data, label=labels)\\nparams = {\\n  'max_depth': 2,\\n  'lambda': 1.5,\\n  'objective':'binary:logistic'\\n}\\ncv_results = xgb.cv(params, dtrain, num_boost_round=5)\\nprint('CV Results:\\n{}'.format(cv_results))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# predefined data and labels\n",
    "dtrain = xgb.DMatrix(data, label=labels)\n",
    "params = {\n",
    "  'max_depth': 2,\n",
    "  'lambda': 1.5,\n",
    "  'objective':'binary:logistic'\n",
    "}\n",
    "cv_results = xgb.cv(params, dtrain, num_boost_round=5)\n",
    "print('CV Results:\\n{}'.format(cv_results))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Boosters\n",
    "\n",
    "We can save our trained Booster objects by using the .save_model() function for the object. This saves the model's binary data into an input file, with a .bin extension. We can restore a trained Booster object using a booster object's .load_model() function (we call the function after creating a new Booster instance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLOADING BOOSTERS: \\n\\n# Load saved Booster\\nnew_bst = xgb.Booster()\\nnew_bst.load_model('model.bin')\\n\\n# Same dpred from before\\nprint('Probabilities:\\n{}'.format(\\n  repr(new_bst.predict(dpred))))\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SAVING BOOSTERS:\n",
    "\n",
    "# predefined data and labels\n",
    "dtrain = xgb.DMatrix(data, label=labels)\n",
    "params = {\n",
    "  'max_depth': 3,\n",
    "  'objective':'binary:logistic'\n",
    "}\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "# 2 new data observations\n",
    "dpred = xgb.DMatrix(new_data)\n",
    "print('Probabilities:\\n{}'.format(\n",
    "  repr(bst.predict(dpred))))\n",
    "\n",
    "bst.save_model('model.bin')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "LOADING BOOSTERS: \n",
    "\n",
    "# Load saved Booster\n",
    "new_bst = xgb.Booster()\n",
    "new_bst.load_model('model.bin')\n",
    "\n",
    "# Same dpred from before\n",
    "print('Probabilities:\\n{}'.format(\n",
    "  repr(new_bst.predict(dpred))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression with XGBoost\n",
    "\n",
    "XGBoost also provides a wrapper that functions like the sklearn models. It functions in the same way as boosters, but does so in a more familiar syntax. For classification, the XGBoost wrapper is XGBClassifier(). For regression, the XGBoost wrapper is XGBRegressor(). Like regular scikit-learn models, it can be trained with a simple call to fit with NumPy arrays as input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = xgb.XGBClassifier()\n",
    "# predefined data and labels\n",
    "model.fit(data, labels)\n",
    "\n",
    "# new_data contains 2 new data observations\n",
    "predictions = model.predict(new_data)\n",
    "print('Predictions:\\n{}'.format(repr(predictions)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters for the original Booster object are now keyword arguments for the XGBClassifier. For instance, we can specify the type of classification, i.e. the 'objective' parameter for Booster objects, with the objective keyword argument (the default is binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = xgb.XGBClassifier(objective='multi:softmax')\\n# predefined data and labels (multiclass dataset)\\nmodel.fit(data, labels)\\n\\n# new_data contains 2 new data observations\\npredictions = model.predict(new_data)\\nprint('Predictions:\\n{}'.format(repr(predictions)))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = xgb.XGBClassifier(objective='multi:softmax')\n",
    "# predefined data and labels (multiclass dataset)\n",
    "model.fit(data, labels)\n",
    "\n",
    "# new_data contains 2 new data observations\n",
    "predictions = model.predict(new_data)\n",
    "print('Predictions:\\n{}'.format(repr(predictions)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Not every feature is equally important in helping a boosted tree make decisions. Certain features are more important than others. \n",
    "\n",
    "We can view the relative/proportional importance of each dataset feature using the feature_importances_ property of the model. \n",
    "\n",
    "By default, the plot_importance() function looks at feature weight as the importance metric (i.e., how often does the feature appear in the boosted decision tree?). You can change to a different importance metric with the \"importance_type\" keyword arg. We can set, for example, importance_type equal to 'gain', which means that we use information gain as the importance metric. Information gain is a commonly used metric for determining how good a feature is at differentiating the dataset, which is important in making predictions with a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# plot of importances\\n\\nmodel = xgb.XGBRegressor()\\n# predefined data and labels (for regression)\\nmodel.fit(data, labels)\\n\\nxgb.plot_importance(model, importance_type='gain')\\nplt.show() # matplotlib plot\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = xgb.XGBClassifier()\n",
    "# predefined data and labels\n",
    "model.fit(data, labels)\n",
    "\n",
    "# Array of feature importances\n",
    "print('Feature importances:\\n{}'.format(\n",
    "  repr(model.feature_importances_)))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "\n",
    "Feature importances:\n",
    "array([0.17941953, 0.11345647, 0.41556728, 0.29155672], dtype=float32)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# plot of importances\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "# predefined data and labels (for regression)\n",
    "model.fit(data, labels)\n",
    "\n",
    "xgb.plot_importance(model, importance_type='gain')\n",
    "plt.show() # matplotlib plot\n",
    "\"\"\"\n",
    "\n",
    "# The resulting plot is a bar graph of the F-scores ( F1-scores) for each feature\n",
    "# (the number next to each bar is the exact F-score). Note that the features are \n",
    "# labeled as \"fN\", where N is the index of the column in the dataset. The F-score \n",
    "# is a standardized measurement of a feature's importance, based on the specified importance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with grid-search cross-validation\n",
    "\n",
    "We can use sklearn's GridSearchCV in order to perform hyperparameter tuning. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = xgb.XGBClassifier()\\nparams = {'max_depth': range(2, 5)}\\n\\nfrom sklearn.model_selection import GridSearchCV\\ncv_model = GridSearchCV(model, params, cv=4, iid=False)\\n\\n# predefined data and labels\\ncv_model.fit(data, labels)\\nprint('Best max_depth: {}\\n'.format(\\n  cv_model.best_params_['max_depth']))\\n\\n# new_data contains 2 new data observations\\nprint('Predictions:\\n{}'.format(\\n  repr(cv_model.predict(new_data))))\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = xgb.XGBClassifier()\n",
    "params = {'max_depth': range(2, 5)}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cv_model = GridSearchCV(model, params, cv=4, iid=False)\n",
    "\n",
    "# predefined data and labels\n",
    "cv_model.fit(data, labels)\n",
    "print('Best max_depth: {}\\n'.format(\n",
    "  cv_model.best_params_['max_depth']))\n",
    "\n",
    "# new_data contains 2 new data observations\n",
    "print('Predictions:\\n{}'.format(\n",
    "  repr(cv_model.predict(new_data))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading XGBoost models\n",
    "\n",
    "As is the case with other sklearn models, we can save and load them with the joblib API. \n",
    "\n",
    "Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom joblib import dump, load\\ndump(clf, 'filename.joblib') \\nclf = load('filename.joblib') \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from joblib import dump, load\n",
    "dump(clf, 'filename.joblib') \n",
    "clf = load('filename.joblib') \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
