{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Here, we'll get an introduction into Keras. Keras is a simpler alternative to Tensorflow, and is built on top of Tensorflow. Because of this, its API is considerably easier to manage. \n",
    "\n",
    "In this section, we'll use Keras to build a multilayer perceptron for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model in Keras\n",
    "\n",
    "Every neural network model in Keras is an instance of the Sequential() object. This acts as the container for the neural network, allowing us to build the model by stacking multiple layers into the Sequential() object. Moreover, this wrapper eliminates the need to instantiate a graph by hand in Tensorflow, which is not done using the same object-oriented procedure as is the case in many Python applications (there's no \"graph\" object, for example, that you maintain. Instead, in Tf, you instantiate different parts of the graph, but there's no \"graph\" object). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Creation\n",
    "\n",
    "When building a model, we start off by initializing a Sequential object. \n",
    "\n",
    "We can initialize an empty Sequential object and add layers onto the model using the add function, or we can directly initialize the Sequential object with a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: had to import each of these components (e.g., Sequential, Dense)\n",
    "\n",
    "# here, we're creating a 4-layer neural network\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# create first layer, add to model\n",
    "layer1 = Dense(5, input_dim = 4) # same as tf.layers.dense, but with considerably less setup\n",
    "model.add(layer1)\n",
    "layer2 = Dense(3, activation = 'relu') # note: Keras seems to abstract away a lot of setup, such as dimension matching\n",
    "model.add(layer2)\n",
    "\n",
    "\"\"\"\n",
    "Alternate way to instantiate:\n",
    "\n",
    "layer1 = Dense(5, input_dim=4)\n",
    "layer2 = Dense(3, activation='relu')\n",
    "model = Sequential([layer1, layer2])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Output\n",
    "\n",
    "When we built the MLP (multiclass perceptron), each model produced logits. This is because the Tensorflow cross-entropy loss functions applied the sigmoid/softmax function to the output of the MLP (that is, the logits)\n",
    "\n",
    "In Keras, the cross-entropy loss functions only calculate cross-entropy, without applying the sigmoid/softmax function to the MLP output. As a result, we can have the model directly output class probabilities, rather than just logits, by directly applying sigmoid/softmax activation functions and having them output class probabilities as a the output of the network (as opposed to the Tensorflow implementation, when the output was a logit and an extra step was required, outside of the network, to convert the logits into class probabilities using sigmoid/softmax)\n",
    "\n",
    "In essence, Keras lets us skip one step of the pipeline. In Tensorflow, the output of the network had to be logits since the cross-entropy loss functions (which are needed in order to optimize the network) applies the sigmoid/softmax function to the output in addition to calculating cross-entropy. This means that in Tensorflow, you have to apply the sigmoid/softmax function after the MLP is finished, and your initial logit output isn't useful except for the fact that it was an intermediate step. However, in Keras, this intermediate step is taken out. Since cross-entropy loss functions in Keras only calculate cross-entropy, there's no need to keep the logits as an intermediate outcome, so you can directly transform the logit outputs, using a sigmoid/softmax function, in the output layer, so that the output from the neural network is not a set of logits but rather a set of class probabilities. \n",
    "\n",
    "Below are two examples of the Keras implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# creating an MLP model for binary classification (with a sigmoid activation function). \n",
    "# here, the output is the direct probabilities, since we pass in the output to a sigmoid function, rather\n",
    "# than returning the logits and then, in a separate step, adding a sigmoid function. \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "layer1 = Dense(5, activation='relu', input_dim=4)\n",
    "model.add(layer1)\n",
    "layer2 = Dense(1, activation='sigmoid')\n",
    "model.add(layer2)\n",
    "\n",
    "# Creating an MLP model for multiclass classification with 3 classes (softmax activation).\n",
    "model = Sequential()\n",
    "layer1 = Dense(5, input_dim=4)\n",
    "model.add(layer1)\n",
    "layer2 = Dense(3, activation='softmax')\n",
    "model.add(layer2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration (i.e., getting the Keras model up and running)\n",
    "\n",
    "Configuring a Keras model is really simple. In Tensorflow, you had to instantiate a session, initialize variables, have the graph set up, etc. In Keras, you just call the 'compile' function and it's good to go. \n",
    "\n",
    "When we compile the model, it means that we have our neural network instantiated/set up, with the necessary layers and activation functions and whatnot. Now it's ready to take in any input values that we'd like to run through the model. \n",
    "\n",
    "The function takes in a single required argument, which is the required optimizer (e.g., Adam). A shorthand way to specify the optimizer is to use a (lowercase) string of the optimizer's name (e.g. 'adam', 'sgd', 'adagrad').\n",
    "\n",
    "The two main keyword arguments are 'loss' and 'metric'. The 'loss' parameter is what we're trying to optimize for during each run of our model (it's what we're trying to minimize). The 'metric' parameter is how we'll evaluate the model's performance both during training (so, for example, we can print the accuracy after each round) and at the end. \n",
    "\n",
    "The two main keyword arguments to know are loss and metrics. The loss keyword argument specifies the loss function to use. For binary classification, we set the value to 'binary_crossentropy', which is the binary cross-entropy function. For multiclass classification, we set the value to 'categorical_crossentropy', which is the multiclass cross-entropy function.\n",
    "\n",
    "The metrics keyword argument takes in a list of strings, representing metrics we want to track during training and evaluation. For classification, we only need to track the model loss (which is tracked by default) and the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model:\n",
    "model = Sequential()\n",
    "\n",
    "# add layers\n",
    "layer1 = Dense(5, activation = 'relu', input_dim = 4)\n",
    "model.add(layer1)\n",
    "layer2 = Dense(1, activation = 'sigmoid')\n",
    "model.add(layer2)\n",
    "\n",
    "# compile model (configure it for training)\n",
    "model.compile('adam', \n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Execution\n",
    "\n",
    "Now that our neural network is set up, we can run data through it. \n",
    "\n",
    "In Tensorflow, this involved running the session and keeping it running. \n",
    "\n",
    "However, in Keras, it's as simple as using a .fit() function \n",
    "\n",
    "The first two arguments of the fit function are the input data and labels. Unlike in Tensorflow, where the inputs had to all be tensors, the inputs to the Keras model can be numpy arrays. The batch size can be specified using the 'batch_size' argument (default = 32). The number of epochs (complete passes through all the data) can be specified through the 'epoch' argument (default = 1)\n",
    "\n",
    "The output of the training is logged for each epohc. Since we configured the model to track classification accuracy, both the loss and classification accuracy per epoch are measured. The output of the fit function is a History object, and this records the training metrics. The object's history attribute is a dictionary that contains the metric values at each epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# define network\n",
    "model = Sequential()\n",
    "layer1 = Dense(200, activation='relu', input_dim=4)\n",
    "model.add(layer1)\n",
    "layer2 = Dense(200, activation='relu')\n",
    "model.add(layer2)\n",
    "layer3 = Dense(3, activation='softmax')\n",
    "model.add(layer3)\n",
    "\n",
    "# compile\n",
    "model.compile('adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# predefined multiclass dataset\n",
    "train_output = model.fit(data, labels,\n",
    "                         batch_size=20, epochs=5)\n",
    "\n",
    "# get results\n",
    "print(train_output.history)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "To evaluate the performance of a model, we use the Sequential object's .evaluate() function, which takes in the data and labels as its first two arguments. The function will evaluate the model over the entire input data and labels, and will return a list containing the evaluation loss as well as values for each of the metrics specified during model configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(model.evaluate(eval_data, eval_labels))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(model.evaluate(eval_data, eval_labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction\n",
    "\n",
    "Using the Sequential object's .predict() function, you can pass in a dataset (even a Numpy dataset), and the output of the predict function is the output of the model. For classification, the predict function is the model's class probabilities for each data observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print('{}'.format(repr(model.predict(new_data))))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
