{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning with Tensorflow\n",
    "\n",
    "Tensorflow is a deep learning framework. Its name comes from its use of tensors, which are multidimensional (i.e., generalized) vectors and matrices. \n",
    "\n",
    "When we use Tensorflow, we first create the computational graph structure. Then, we train and evaluate the model with input data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "Before we input any data, we first have to set up the computational graph. To do so, we can use tf.placeholder. tf.placeholder acts as a \"placeholder\" for the input data and labels. Without the tf.placeholder, we would not be able to train our model on real input data. \n",
    "\n",
    "To instantiate tf.placeholder, you need to specify (1) data type  (2) shape, and (3) name\n",
    "\n",
    "For data type, you can specify something like tf.int32 or tf.float32 or something like that\n",
    "\n",
    "For shape, the shape argument is a tuple of integers that represents the size of each of the placeholder's tensor dimensions. Typically, the data is 2-D, with rows = observations, columns = features\n",
    "\n",
    "For name, we can give our placeholder a custom name. \n",
    "\n",
    "When we specify sizes in Tf, we can use None in place of a dimension size. When we use None in the shape tuple, we allow that dimension to take any size (though we have to specify the other dimension(s)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_inputs(input_size):\n",
    "    # example of instantiating a placeholder, creating the computational graph\n",
    "    inputs = tf.placeholder(tf.float32, shape = (None, input_size), name = 'inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Perceptron\n",
    "\n",
    "A single layer perceptron is the most basic form of neural network. In this type of network, the inputs are directly connected to the outputs. The input layer (i.e., self.inputs) is directly connected to the output layer, which has 'output_size' # of neurons. For a fully-connected single-layer perceptron, each of the input neurons is connected to each of the output neurons. \n",
    "\n",
    "To implement a fully-connected layer, we use tf.layers.dense. It takes in a neuron layer and output size as required arguments, and adds a fully-connected output layer with the given size to the computation graph. \n",
    "\n",
    "In addition to the input layer neurons, tf.layers.dense adds another neuron called the bias, which always has a value of 1 and has full connections to the output layer.\n",
    "\n",
    "This lets you tinker with the weights (which come from the weights of the edges that connect the input and output layers) and bias (which is accounted for by the additional bias neuron)\n",
    "\n",
    "Imagine this to be a fully connected graph. The input nodes have certain values. They are connected to the output node(s), and the edges that connect them have certain weights. In a single layer perceptron, the linear combination of weights + node values, for all the nodes that connect to the output, plus the bias (which comes from an additional node whose edge has weight 1), should approximate the true value. \n",
    "\n",
    "For a single layer perceptron with one output node, the output of the fully connected layer is a linear combination of the input neuron values. For a single layer perceptron with 3 input nodes and 1 output node, it would look like the following:\n",
    "\n",
    "$$ (\\omega_1 • x_1) + (\\omega_2) • x_2) + (\\omega_3 • x_3) + bias = Output$$\n",
    "\n",
    "Where the $\\omega_i$ come from the weights on the edges connecting the input neurons to the output neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and Logits\n",
    "\n",
    "When we use a single-layer perceptron for classification, we output logits (log-odds). Logits map a probability [0, 1] to a real number. When output_size = 1, our model outputs a single logit per data point. The logits will then be converted to probabilities representing how likely it is for the data point to be labeled 1 (as opposed to 0)\n",
    "\n",
    "Logit function: $f(x) = log\\frac{x}{1-x}$. The x-values here range from 0 to 1 (since x > 1 leads to log of a negative number, which isn't possible). To get probabilities from logit, take inverse\n",
    "\n",
    "Upon simplification, given x = a logit function, we can transform it to a probability between 0 and 1 with the following function: $f(x) = \\frac{e^x}{1 + e^x}$ (known as a sigmoid function)\n",
    "\n",
    "So, in a single layer perceptron with one output, in the context of classification, the output is a logit. The logit can then be translated to a probability between 0 and 1. \n",
    "\n",
    "We want our neural network to produce logits with large absolute values, since this pushes our resulting probabilities to be 0 or 1, thus allowing us to be pretty sure of our guesses (as opposed to a p = 0.4 or 0.6, which could realistically go either way).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_layers(inputs, output_size):\n",
    "    # here, we create a dense, fully connected layer, with one output. \n",
    "    logits = tf.layers.dense(inputs, output_size, name = 'logits')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating neural network performance\n",
    "\n",
    "#### Converting logits to probabilities\n",
    "\n",
    "We saw above that the single layer perceptron returned logits. To turn logits into probabilities $\\in [0, 1]$, we use the sigmoid function $f(x) = \\frac{e^x}{1 + e^x}$\n",
    "\n",
    "We can implement this in Tensorflow with tf.nn.sigmoid(). \n",
    "\n",
    "In the context of binary classification, the output of tf.nn.sigmoid represents the probability that the input belongs to class 1 (as opposed to 0). A probability closer to 0 or 1 means the model is pretty sure in its prediction, while a probability closer to 0.5 means the model is unsure (0.5 is equivalent to a random guess of the label's value).\n",
    "\n",
    "#### Predictions and accuracy\n",
    "\n",
    "In the case of binary classification, if we just round our probabilities to the nearest integer we would get our predictions (0 or 1). Then our prediction accuracy would be the number of correct predictions divided by the number of labels. In Tensorflow, we can use the function tf.reduce_mean(), which produces the overall mean of a tensor's numeric values. In the context of binary classification, we can use this to calculate prediction accuracy as the mean number of correct predictions across all input data points.\n",
    "\n",
    "We use these metrics to evaluate how good our model is both during and after training. This way we can experiment with different computation graphs, such as different numbers of neurons or layers, and find which structure works best for our model.\n",
    "\n",
    "Note: In the example below, our rounded probabilities are of type tf.float32, but our labels are tf.int32. We need the types to match to compare the two. We can fix this problem using tf.cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# taking logits from above, turn into probability \n",
    "probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "# get predictions (but here, as tf.float32)\n",
    "rounded_probs = tf.round(probs)\n",
    "\n",
    "# get predictions as int32\n",
    "predictions = tf.cast(rounded_probs, tf.int32)\n",
    "\n",
    "# get accuracy\n",
    "is_correct = tf.equal(predictions, labels)\n",
    "\n",
    "# we can get the mean using tf.reduce_mean, but we need to use a float for this, so we cast\n",
    "is_correct_float = tf.cast(is_correct, tf.float32)\n",
    "accuracy = tf.reduce_mean(is_correct_float)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "How do we optimize neural network weights? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is training and what is its purpose?\n",
    "\n",
    "Each edge connecting an input node to the output node has a particular weight assigned. We need to train the neural network so that our weights are optimized. \n",
    "\n",
    "To train a neural network, we need a loss function, which tells us \"how far off\" our predictions were from the right answer. \n",
    "\n",
    "For regression, the L1 norm ( $\\sum\\limits_{i} |\\hat{y} - y|$ )and L2 norm ( $\\sum\\limits_{i} (\\hat{y} - y)^2$  ) are common loss functions. \n",
    "\n",
    "For classification, we want a loss function that is small when the probability is close to the label (i.e. a probability of 0.99 for a label of 1) and large when the probability is far from the label (i.e. a probability of 0.99 for a label of 0). We don't want to use just accuracy, since we want to (1) have accurate results, but also (2) have probabilities that are near 0 and 1, so that our guesses are more certain. To achieve this, we use $\\textbf{cross entropy}$ (which essentially is how close our distribution of \"predicted\" probabilities matches that of the true probabilities - the better they align, the lower our cross entropy is)\n",
    "\n",
    "Our goal is to minimize our cross entropy based on the model's logits and labels. We do this through gradient descent, and use backpropagation to find the optimal gradient for the model to follow. \n",
    "\n",
    "In Tensorflow, gradient descent is implemented as an object, with tf.train.GradientDescentOptimizer. \n",
    "\n",
    "You can use a variety of optimizers (one example being GradientDescentOptimizer, another popular one being Adam - implemented with tf.train.AdamOptimizer). For each of these optimizers, we can set a learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# cast labels as float, since logits are floats\\nlabels_float = tf.cast(labels, tf.float32)\\n\\n# in TF, converting logits to probabilities using sigmoid and evaluating with cross entropy is so common, \\n# there's a function to combine the operations\\ncross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels_float, logits = logits)\\n\\n# Since cross_entropy represents the sigmoid cross entropy for each input data label \\n# (so its shape is (None, 1)), we'll use the overall mean of the cross entropy as our loss.\\nloss = tf.reduce_mean(cross_entropy)\\n\\n# now, we'll use the Adam optimizer\\nadam = tf.train.AdamOptimizer() # create instance of AdamOptimizer\\ntrain_op = adam.minimize(loss) # minimize loss function\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# cast labels as float, since logits are floats\n",
    "labels_float = tf.cast(labels, tf.float32)\n",
    "\n",
    "# in TF, converting logits to probabilities using sigmoid and evaluating with cross entropy is so common, \n",
    "# there's a function to combine the operations\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels_float, logits = logits)\n",
    "\n",
    "# Since cross_entropy represents the sigmoid cross entropy for each input data label \n",
    "# (so its shape is (None, 1)), we'll use the overall mean of the cross entropy as our loss.\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# now, we'll use the Adam optimizer\n",
    "adam = tf.train.AdamOptimizer() # create instance of AdamOptimizer\n",
    "train_op = adam.minimize(loss) # minimize loss function\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Before, we went through the steps in one run of the neural network, from creating a graph to building a fully connected single layer perceptron to getting the logits --> probabilities to evaluating the results to optimizing. \n",
    "\n",
    "Now, let's see how to train a Tensorflow neural network using actual data. \n",
    "\n",
    "We first initialize our graph and our variables.\n",
    "\n",
    "Then, we call tf.Session(), which means that we can start inputting actual data into our Tensorflow session. \n",
    "\n",
    "In tf.Session(), we have access to the 'run' function, which is what actually allows us to train and/or evaluate our model on real input data. Prior to that step, we just built the computational graph (i.e., the layers and operations). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 'run'\n",
    "\n",
    "The required argument for 'run' is typically either a single tensor/operation or a list/tuple of tensors/operations. Calling 'run' on the tensor returns the value of the tensor after executing the computational graph. The output of run with a tensor input is a NumPy array\n",
    "\n",
    "Below are some examples of running sess.run() on constant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code only works if you instantiate a graph first (also, it requires using tf.compat.v1 in lieu of tf.)\n",
    "\n",
    "t = tf.constant([1, 2, 3])\n",
    "sess = tf.Session()\n",
    "arr = sess.run(t)\n",
    "print('{}\\n'.format(repr(arr)))\n",
    "\n",
    "t2 = tf.constant(4)\n",
    "tup = sess.run((t, t2))\n",
    "print('{}\\n'.format(repr(tup)))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT: \n",
    "\n",
    "array([1, 2, 3], dtype=int32)\n",
    "\n",
    "(array([1, 2, 3], dtype=int32), 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, this is TensorFlow'\n"
     ]
    }
   ],
   "source": [
    "# initialize session\n",
    "s = tf.compat.v1.Session()\n",
    "with tf.compat.v1.get_default_graph().as_default():\n",
    "    h = tf.constant('Hello, this is TensorFlow')\n",
    "    # run input through tensor graph\n",
    "    print(s.run(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'run' is what actually allows you to run data through a neural network. \n",
    "\n",
    "You create the computational graph structure, with all the weights and nodes predefined (using lots of tf.placeholder() tensors). Then, you create a session, which means that you're ready to run actual data through the graph. While the session is running, you can use sess.run(x), where x is the tensor that you'd like to pass through the computational graph that you previously defined. \n",
    "\n",
    "So, creating a neural network has two parts: (1) design the graph structure, and (2) run the input through the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'run' has several keyword arguments, the most important being 'feed_dict'. Each key is a tensor from the model's computation graph. The key's value can be a Python scalar, list, or NumPy array. We can use feed_dict to pass values into certain tensors of the computational graph. Each tf.placeholder object used in the model execution must be used as a key in the feed_dict, with the value and shape matching that of the placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "feed_dict = {\n",
    "  inputs: [[1.1, -0.3],\n",
    "           [0.2, 0.1]]\n",
    "}\n",
    "sess = tf.Session()\n",
    "arr = sess.run(inputs, feed_dict=feed_dict)\n",
    "print('{}\\n'.format(repr(arr)))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT: \n",
    "\n",
    "array([[ 1.1, -0.3],\n",
    "       [ 0.2,  0.1]], dtype=float32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing variables\n",
    "\n",
    "Whenever we call 'run' on the graph, every tensor in the graph must either have a value or have been fed a value through feed_dict. \n",
    "\n",
    "But, when we're first starting the session, our graph doesn't have its weights and variables initialized. Therefore, we need to initialize our weights, using tf.global_variables_initializer(). We can then feed the initializer into the run session, and this initializes all the variables in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "feed_dict = {\n",
    "  inputs: [[1.1, -0.3],\n",
    "           [0.2, 0.1]]\n",
    "}\n",
    "\n",
    "# initialized variables are part of dense layers\n",
    "logits = tf.layers.dense(inputs, 1, name='logits')\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op) # variable initialization\n",
    "arr = sess.run(logits, feed_dict=feed_dict)\n",
    "print('{}\\n'.format(repr(arr)))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "More updated way to do it: \n",
    "\n",
    "# Add an Op to initialize global variables.\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Run the Op that initializes global variables.\n",
    "    sess.run(init_op)\n",
    "    # ...you can now run any Op that uses variable values...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training steps\n",
    "\n",
    "The num_steps argument represents the number of iterations we use to train our model. Each iteration we train the model on a batch of data points. So input_data is essentially a large dataset divided into chunks (i.e. batches), and each iteration we train on a specific batch of points and their corresponding labels.\n",
    "\n",
    "The batch size determines how the model trains. Larger batch sizes usually result in faster training but less accurate models (since having more batches means fewer observations per batch), and vice-versa for smaller batch sizes. Choosing the batch size is a speed-precision tradeoff.\n",
    "\n",
    "When training a neural network, it's usually a good idea to print out the loss every so often, so you know that the model is training correctly and to stop the training when the loss has converged to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# initialize session, initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "# each iteration of the for loop feeds the ith data observation and label into the model\n",
    "for i in range(1000):\n",
    "    # define data to feed into graph. There's already placeholders, so we\n",
    "    # just plug in this data into the placeholders\n",
    "    feed_dict = {inputs : input_data[i], labels : input_labels[i]}\n",
    "    # the session runs, on the data fed into it by feed_dict\n",
    "    sess.run(train_op, feed_dict = feed_dict)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "\n",
    "We evaluate a model's performance using a test set. It's good practice to split data into three sets: (1) training set (80% of observations, used for creating the model), (2) validation set (10% of observations, used for evaluating the model between training runs), and (3) test set (10% of observations, used to evaluate the final model, using some accuracy metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example way of evaluating performance (assume that necessary variables are pre-defined)\n",
    "\n",
    "feed_dict = {inputs : test_data, labels : test_labels}\n",
    "eval_acc = sess.run(accuracy, feed_dict = feed_dict)\n",
    "\n",
    "# notice that everything in the session is accomplished with sess.run\n",
    "# also, accuracy was defined above as having used the function tf.reduce_mean\n",
    "# here, it seems like accuracy was one of the predefined objects declared when setting up the computational graph\n",
    "# but, when you call sess.run and output the return value, then the model explicitly calculates the accuracy\n",
    "# and returns it, for the input in feed_dict\n",
    "\n",
    "# plus here, since it is being run on the test set, it seems like what the function is saying is \"run this data\n",
    "# through the network, pass in the data in feed_dict, and return the value for \"accuracy\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of a Single Layer Perceptron Model\n",
    "\n",
    "The model we've created, a fully connected single layer perceptron, gets its output values from some linear combination: \n",
    "\n",
    "\n",
    "$$ (\\omega_1 • x_1) + (\\omega_2) • x_2) + (\\omega_3 • x_3) + bias = Output$$\n",
    "\n",
    "However, since it is a linear combination, then the resulting decision boundary (output that leads to p = 0.5) is linear in nature as well. \n",
    "\n",
    "This works if the values can be separated by a linear boundary. However, if they cannot, then using a simple single layer perceptron won't be sufficient.\n",
    "\n",
    "In that case, the model would underperform. However, it wouldn't underperform due to some fault in the training. Instead, it would underperform because, with just a single layer perceptron model, the model can only learn linear decision boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Beyond Single Layer Perceptron Model: Using Hidden Layers\n",
    "\n",
    "If a single linear layer can't model a non-linear boundary (e.g., if the boundary were a circle, such as $x^2 + y^2 = r^2$, no straight line can replicate that), then the answer is to add more linear boundaries and/or use non-linearities. \n",
    "\n",
    "We just need the correct linear + nonlinear combinations that will give us the output that we want, which can look like adding more layers or changing the output of each layer.\n",
    "\n",
    "Here, we'll expand on the single layer perceptron by adding a \"hidden\" layer between the input and output layers. We can, for example, add 5 neurons to the hidden layer, creating a multilayer perceptron with 3 neurons in the input layer, 5 neurons in the hidden layer, and 1 neuron in the output layer. \n",
    "\n",
    "For very complex datasets, a model could have multiple hidden layers with hundreds of neurons per layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding non-linearities\n",
    "\n",
    "We add non-linearities to our model through \"activation functions\". \n",
    "\n",
    "At its core, a neuron in Layer 1 is connected to a neuron in Layer 2 through an edge with an associated weight, and the input from Layer 1 is sent to Layer 2 via a linear combination of (weight * input) + bias. So, this part of it stays linear and must stay that way. However, you can introduce non-linearities through activation functions, which take the output from (weight * input) + bias and modifies it in some non-linear way. \n",
    "\n",
    "This is useful because without activation functions, the inputs would move from layer to layer through only linear combinations, and some boundaries are not possible (or, very difficult) to replicate with only linear borders. \n",
    "\n",
    "The three most common activation functions used are ReLU, tanh, and sigmoid. Each has its own uses, but ReLU is the most commonly used. \n",
    "\n",
    "$$ReLU = max(0, x)$$\n",
    "\n",
    "This function looks pretty linear, doesn't it? \n",
    "\n",
    "Yes, it does look pretty linear, but it can do a good job in approximating non-linear lines. \n",
    "\n",
    "For example, imagine that there were 4 nodes in the hidden layer, and the output from each of the nodes in the hidden layer was passed through an activation function and then sent to the output node. Then, the output node would receive a linear combination of the outputs of the 4 nodes in the hidden layer. The output could be of the form (where the argument for ReLU is what's passed into the activation function of that node. That argument is the sum of all the other linear combinations of weights + biases that it receives from the input nodes that it is connected to):\n",
    "\n",
    "$$f(x) = ReLU (x) + ReLU (-x) + ReLU (-2x + 2) + ReLU (2x + 2) $$.\n",
    "\n",
    "\n",
    "Because of the ReLU function, the output of these values would look like a parabola\n",
    "\n",
    "With enough linear combinations and ReLU transformations, a model could easily learn the quadratic transformation. \n",
    "\n",
    "The simplicity of the ReLU model allows it to avoid the vanishing gradient problem (where weights are normally updated by the partial derivative of the error in the direction of that weight, but if the gradient is sufficiently small, it won't allow the weight to change its value). Plus, the fact that it maps negative values to 0 means that the model actually trains faster and avoids overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_layers(inputs, output_size): ## input --> hidden layer --> output\n",
    "    # create hidden layer\n",
    "    hidden1 = tf.layers.dense(inputs, 5, activation = tf.nn.relu, name = 'hidden1')\n",
    "    # create output layer\n",
    "    logits = tf.layers.dense(hidden1, output_size,\n",
    "                           name='logits')\n",
    "    # return logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "To deal with multiclass classification, we take the output layer and we one-hot-encode for each of the class labels. So, for example, if the output was [cat, dog, mouse], we'd have separate indicator columns for cat, dog, and mouse. \n",
    "\n",
    "You can think of this as multiple binary classification, where the actual class of the data point is labeled as 1 (True), while the other classes are labeled as 0 (False).\n",
    "\n",
    "Since now there are multiple decision boundaries, it would help to either add additional layers, increase the size of the current layer, or introduce more nonlinearities. However, adding an extra hidden layer may decrease the number of training iterations needed for convergence compared to maintaining a single hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding on the number of hidden layers\n",
    "\n",
    "When deciding how many hidden layers a model needs (i.e. how deep it is) and how many neurons are at each hidden layer, it is a good idea to base the decision on the problem itself. There are a few general rules of thumb, but they do not apply to every scenario. For example, it is common not to need more than 3 hidden layers in a neural network, but if you are working on a complicated problem you would most likely need more (Google's Alpha Go needed more than a dozen layers).\n",
    "\n",
    "Try at first with the minimal number of hidden layers, and then add as you need it. Only add extra layers/neurons if you need it. Smaller models train faster and their accuracies and performances are easier to diagnose, since there's less \"black box\" content. It then becomes easier to optimize the number of layers and neurons in your model through experimentation.\n",
    "\n",
    "Plus, the more layers and neurons your model has, the more likely it can overfit, since it has a greater capacity to pick up more nuanced aspects of the data, which might be unique to that dataset and doesn't actually generalize (e.g., if you're classifying cars on the street but it happens to be the case that there's a watermark on the bottom right of all the pictures that have cars, a neural network with too many layers might pick up on something as trivial as this and say that any picture that it sees with a watermark is immediately a car). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a neural network with 2 hidden layers: input --> hidden1 --> hidden2 --> output\n",
    "def model_layers(input_, output_size):\n",
    "    # define first hidden layer. Note: tf.layers.dense creates a fully connected graph, where the nodes of the input\n",
    "    # layer are all connected with the nodes of the hidden layer\n",
    "    hidden1 = tf.layers.dense(inputs, 5, \n",
    "                              activation = tf.nn.relu, \n",
    "                              name = 'hidden1')\n",
    "    # create second hidden layer\n",
    "    hidden2 = tf.layers.dense(hidden1, 5, \n",
    "                              activation = tf.nn.relu, \n",
    "                              name = 'hidden2')\n",
    "    # create output\n",
    "    logits = tf.layers.dense(hidden2, output_size, name = 'logits') # no activation function = takes linear combination\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the softmax function for multiclass classification to transform logits to probabilities\n",
    "\n",
    "Previously, in the case of binary classification, we used the sigmoid function to transform the logits into probabilities. However, for multiclass classification, we have to use a generalization of the sigmoid function, known as the softmax function. \n",
    "\n",
    "The softmax function takes a vector of M classes and normalizes it into a probability distribution with M probabilities, proportional to the exponentials of the input numbers. After applying softmax, each entry will be in the interval (0, 1), and all the probabilities will add up to 1. The larger input components will correspond to larger probabilities. \n",
    "\n",
    "We apply the standard exponential function to each class m $\\in$ M and normalize these values by dividing by the sum of all these exponentials. The softmax function takes the following form:\n",
    "\n",
    "$$\\sigma(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum\\limits_{j= 1}^K e^{z_j}}, i = 1 , ... , K ; \\textbf{z} = (z_1, ... z_K) \\in \\mathbb{R}^K$$\n",
    "\n",
    "Each class probability is proportion to how large its logit relative to the sum of all the other class logits\n",
    "\n",
    "Note: we also have to replace the sigmoid cross-entropy with softmax cross-entropy (see below). The cross-entropy is now calculated for each class and then averaged at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# applying softmax function \\n\\nt = tf.constant([[0.4, -0.8, 1.3],\\n                 [0.2, -1.2, -0.4]])\\nsoftmax_t = tf.nn.softmax(t)\\nsess = tf.Session()\\nprint('{}\\n'.format(repr(sess.run(t))))\\nprint('{}\\n'.format(repr(sess.run(softmax_t))))\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# applying softmax function \n",
    "\n",
    "t = tf.constant([[0.4, -0.8, 1.3],\n",
    "                 [0.2, -1.2, -0.4]])\n",
    "softmax_t = tf.nn.softmax(t)\n",
    "sess = tf.Session()\n",
    "print('{}\\n'.format(repr(sess.run(t))))\n",
    "print('{}\\n'.format(repr(sess.run(softmax_t))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions with Softmax\n",
    "\n",
    "Now, after using the softmax function, we have a vector of probabilities. Our prediction now becomes a matter of choosing the highest probability (before, it was a matter of just rounding to the nearest integer, since we had to choose between 0 or 1). \n",
    "\n",
    "We need to return the index of the class with the highest probability. We do so with tf.argmax.\n",
    "\n",
    "The function takes in a required input tensor, as well as a few keyword arguments. One of the more important keyword arguments is axis, which tells us which dimension to retrieve the maximum index from. Setting axis=-1 uses the final dimension, which in this case corresponds to retrieving the column index.\n",
    "\n",
    "Below is an implementation of tf.argmax. The prediction for each row of probabilities is just the column index with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "probs = tf.constant([[0.4, 0.3, 0.3],\n",
    "                     [0.2, 0.7, 0.1]])\n",
    "preds = tf.argmax(probs, axis=-1)\n",
    "sess = tf.Session()\n",
    "print('{}\\n'.format(repr(sess.run(probs))))\n",
    "print('{}\\n'.format(repr(sess.run(preds))))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "OUTPUT: \n",
    "\n",
    "array([[0.4, 0.3, 0.3],\n",
    "       [0.2, 0.7, 0.1]], dtype=float32)\n",
    "\n",
    "array([0, 1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, our procedure for binary vs. multiclass classification is still similar in many respects. The setup of the graph is similar (input, hidden layer, setup of weights + biases using an initializer). But, there's just more output nodes. Plus, once the results hit the output layer, then the changes become apparent. The output is still a set of logits. But, in binary classification it was enough to use a sigmoid function to transform the logit into a number in the interval [0, 1], and then round this integer to get the classification 0 or 1. However, in the multiclass case, you need to use a softmax function to transform the logits into probabilities (by taking their normalized exponentials) and return, for each observation, the label with the highest probability (using tf.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example procedure:\n",
    "\n",
    "# transform logits to softmax probabilities\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# get predictions by taking argmax\n",
    "predictions = tf.argmax(probs, axis = -1)\n",
    "\n",
    "# convert labels (which were one-hot-encoded, so they're columns of 0s and 1) back into class labels (1 to M)\n",
    "class_labels = tf.argmax(labels, axis = -1)\n",
    "\n",
    "# evaluate accuracy\n",
    "is_correct = tf.equal(predictions, class_labels)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "For optimization (see procedure earlier in this notebook), we replace cross entropy with the following:\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits)\n",
    "\n",
    "So, the optimization procedure would look something like this:\n",
    "\n",
    "# cast labels as float, since logits are floats\n",
    "labels_float = tf.cast(labels, tf.float32)\n",
    "\n",
    "# in TF, converting logits to probabilities using sigmoid and evaluating with cross entropy is so common, \n",
    "# there's a function to combine the operations\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = labels, logits = logits)\n",
    "\n",
    "# Since cross_entropy represents the sigmoid cross entropy for each input data label \n",
    "# (so its shape is (None, 1)), we'll use the overall mean of the cross entropy as our loss.\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# now, we'll use the Adam optimizer\n",
    "adam = tf.train.AdamOptimizer() # create instance of AdamOptimizer\n",
    "train_op = adam.minimize(loss) # minimize loss function\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
