{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be about clustering methods (unsupervised learning) using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors. It can range from 1 (perfectly correlated, same vector projection) to -1 (going in completely opposite directions, if you connected their starting points it would form a completely straight line going in two directions). A value of 0 means that they're orthogonal. \n",
    "\n",
    "It specifically measures the proportional similarity of the feature values between the two data observations (i.e. the ratio between feature columns).\n",
    "\n",
    "Similarity = $cos(\\theta)$ = $\\frac{\\textbf{A} • \\textbf{B}}{||\\textbf{A}|| • ||\\textbf{B}||}$ = \n",
    "$\\frac{\\sum\\limits_{i = 1}^N A_i B_i}{\\sqrt{\\sum\\limits_{i = 1}^N A^2_i}\\sqrt{\\sum\\limits_{i = 1}^N B^2_i}}$\n",
    "\n",
    "We can calculate the cosine similarity using sklearn's cosine_similarity function (in the metrics.pairwise module). It calculates the cosine similarities for pairs of data observations in a single dataset, or pairs of data observations between two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.        ,  0.99992743, -0.99659724, -0.26311741],\n",
      "       [ 0.99992743,  1.        , -0.99751792, -0.27472113],\n",
      "       [-0.99659724, -0.99751792,  1.        ,  0.34174306],\n",
      "       [-0.26311741, -0.27472113,  0.34174306,  1.        ]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = np.array([\n",
    "  [ 1.1,  0.3],\n",
    "  [ 2.1,  0.6],\n",
    "  [-1.1, -0.4],\n",
    "  [ 0. , -3.2]])\n",
    "cos_sims = cosine_similarity(data)\n",
    "print('{}\\n'.format(repr(cos_sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.9993819 ,  0.99973508, -0.91578821],\n",
      "       [ 0.99888586,  0.99993982, -0.9108828 ],\n",
      "       [-0.99308366, -0.9982304 ,  0.87956492],\n",
      "       [-0.22903933, -0.28525359, -0.14654866]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you pass in two datasets, it computes the cosine similarity for pairs of observations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = np.array([\n",
    "  [ 1.1,  0.3],\n",
    "  [ 2.1,  0.6],\n",
    "  [-1.1, -0.4],\n",
    "  [ 0. , -3.2]])\n",
    "data2 = np.array([\n",
    "  [ 1.7,  0.4],\n",
    "  [ 4.2, 1.25],\n",
    "  [-8.1,  1.2]])\n",
    "cos_sims = cosine_similarity(data, data2)\n",
    "print('{}\\n'.format(repr(cos_sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Nearest Neighbors\n",
    "\n",
    "With this approach, we find the k most similar data observations (i.e. neighbors) for a given data observation (where k represents the number of neighbors).\n",
    "\n",
    "In sklearn, we use the NearestNeighbors() object to implement knn. \n",
    "\n",
    "We fit the NearestNeighbors() instance with data, which is then used as a pool of possible neighbors. The kneighbors function takes in new data observations and returns the k nearest neighbors along with their distances. We can adjust the # of neighbors using the n_neighbors argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[7, 0, 4]])\n",
      "\n",
      "array([[0.17320508, 0.24494897, 0.24494897]])\n",
      "\n",
      "array([[7, 0, 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [5.1, 3.5, 1.4, 0.2],\n",
    "  [4.9, 3. , 1.4, 0.2],\n",
    "  [4.7, 3.2, 1.3, 0.2],\n",
    "  [4.6, 3.1, 1.5, 0.2],\n",
    "  [5. , 3.6, 1.4, 0.2],\n",
    "  [5.4, 3.9, 1.7, 0.4],\n",
    "  [4.6, 3.4, 1.4, 0.3],\n",
    "  [5. , 3.4, 1.5, 0.2],\n",
    "  [4.4, 2.9, 1.4, 0.2],\n",
    "  [4.9, 3.1, 1.5, 0.1]])\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# create nearest neighbors, fit to data\n",
    "neighbors = NearestNeighbors(n_neighbors = 3)\n",
    "neighbors.fit(data)\n",
    "\n",
    "# get new data, see where their nearest neighbors are\n",
    "new_obs = np.array([[5. , 3.5, 1.6, 0.3]])\n",
    "dists, knbrs = neighbors.kneighbors(new_obs)\n",
    "\n",
    "# nearest neighbors indexes\n",
    "print('{}\\n'.format(repr(knbrs)))\n",
    "# nearest neighbor distances\n",
    "print('{}\\n'.format(repr(dists)))\n",
    "\n",
    "only_nbrs = neighbors.kneighbors(new_obs,\n",
    "                                 return_distance=False)\n",
    "print('{}\\n'.format(repr(only_nbrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: K-Means Clustering\n",
    "\n",
    "We can use K-Means Clustering, which groups the observations into k clusters, with k centroids, in which the distance from each observation to the centroids is minimized. A cluster's centroid is equal to the average of all the data observations within the cluster.\n",
    "\n",
    "The K-means clustering algorithm is an iterative process. Each iteration, the algorithm will assign each data observation to the cluster with the closest centroid to the observation (using the regular distance metric).\n",
    "\n",
    "Then it updates each centroid to be equal to the new average of the data observations in the cluster. Note that at the beginning of the algorithm, the cluster centroids are either randomly initialized or (better) initialized using the K-means++ algorithm. The clustering process stops when there are no more changes in cluster assignment for any data observation.\n",
    "\n",
    "In sklearn, k-means clustering is implemented using the KMeans() object, which is part of the cluster module. You need to specify n_clusters, the # of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster:\n",
      "array([[ 1.2,  0.6],\n",
      "       [ 2.4,  0.8],\n",
      "       [-1.6,  1.4],\n",
      "       [ 0. ,  1.2]])\n",
      "\n",
      "Centroid:\n",
      "array([0.5, 1. ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting one centroid:\n",
    "\n",
    "cluster = np.array([\n",
    "  [ 1.2, 0.6],\n",
    "  [ 2.4, 0.8],\n",
    "  [-1.6, 1.4],\n",
    "  [ 0. , 1.2]])\n",
    "print('Cluster:\\n{}\\n'.format(repr(cluster)))\n",
    "\n",
    "centroid = cluster.mean(axis=0)\n",
    "print('Centroid:\\n{}\\n'.format(repr(centroid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# predefined data\n",
    "kmeans.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(kmeans.labels_)))\n",
    "\n",
    "# centroids\n",
    "print('{}\\n'.format(repr(kmeans.cluster_centers_)))\n",
    "\n",
    "new_obs = np.array([\n",
    "  [5.1, 3.2, 1.7, 1.9],\n",
    "  [6.9, 3.2, 5.3, 2.2]])\n",
    "# predict clusters\n",
    "print('{}\\n'.format(repr(kmeans.predict(new_obs))))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT: \n",
    "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n",
    "       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0,\n",
    "       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)\n",
    "\n",
    "array([[6.85      , 3.07368421, 5.74210526, 2.07105263],\n",
    "       [5.006     , 3.418     , 1.464     , 0.244     ],\n",
    "       [5.9016129 , 2.7483871 , 4.39354839, 1.43387097]])\n",
    "\n",
    "array([1, 0], dtype=int32)\n",
    "\n",
    "\"\"\"\n",
    "# the labels_ attribute tells us the final cluster assignment of each observation\n",
    "# the cluster_centers_ attribute tells us the location of each centroid\n",
    "# We use the predict function to assign new data observations to one of the clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, regular k-means clustering can be slow. As a result, we can work with batches of the data. We can use mini-batch k-means clustering, which is just k-means clustering on a batch of the data. \n",
    "\n",
    "In sklearn, mini-batch k-means clustering is implemented using the MiniBatchKMeans() object, which works like the KMeans object except that you need to specify batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.cluster import MiniBatchKMeans\\nkmeans = MiniBatchKMeans(n_clusters=3, batch_size=10)\\n# predefined data\\nkmeans.fit(data)\\n\\n# cluster assignments\\nprint('{}\\n'.format(repr(kmeans.labels_)))\\n\\n# centroids\\nprint('{}\\n'.format(repr(kmeans.cluster_centers_)))\\n\\nnew_obs = np.array([\\n  [5.1, 3.2, 1.7, 1.9],\\n  [6.9, 3.2, 5.3, 2.2]])\\n# predict clusters\\nprint('{}\\n'.format(repr(kmeans.predict(new_obs))))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=3, batch_size=10)\n",
    "# predefined data\n",
    "kmeans.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(kmeans.labels_)))\n",
    "\n",
    "# centroids\n",
    "print('{}\\n'.format(repr(kmeans.cluster_centers_)))\n",
    "\n",
    "new_obs = np.array([\n",
    "  [5.1, 3.2, 1.7, 1.9],\n",
    "  [6.9, 3.2, 5.3, 2.2]])\n",
    "# predict clusters\n",
    "print('{}\\n'.format(repr(kmeans.predict(new_obs))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Hierarchical Clustering (generalization of k-means clustering)\n",
    "\n",
    "K-means clustering assumes that the data consists of spherical clusters, but this isn't often the case.\n",
    "\n",
    "To get around this, we can use hierarchical clustering, which allows for clustering of data but doesn't make any assumptions about the data or clustering.\n",
    "\n",
    "There are two approaches to hierarchical clustering: bottom-up (divisive) and top-down (agglomerative). The divisive approach initially treats all the data as a single cluster, then repeatedly splits it into smaller clusters until we reach the desired number of clusters. The agglomerative approach initially treats each data observation as its own cluster, then repeatedly merges the two most similar clusters until we reach the desired number of clusters.\n",
    "\n",
    "In practice, the agglomerative approach is more often used. \n",
    "\n",
    "In sklearn, agglomerative clustering is implemented through the AgglomerativeClustering() object, which is part of the cluster module. We specify the number of clusters with the n_clusters keyword argument.\n",
    "\n",
    "Since agglomerative clustering doesn't use centroids, there's no cluster_centers_ attribute like with kmeans. Also, there's no predict function to make cluster predictions on the new data (kmeans could make predictions because it used the centroids to determine similarity to new observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.cluster import AgglomerativeClustering\\nagg = AgglomerativeClustering(n_clusters=3)\\n# predefined data\\nagg.fit(data)\\n# cluster assignments\\nprint('{}\\n'.format(repr(agg.labels_)))\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "# predefined data\n",
    "agg.fit(data)\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(agg.labels_)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use agglomerative clustering for dimensionality reduction and feature clustering. \n",
    "\n",
    "We can use PCA to perform feature dimensionality reduction on datasets. But, with agglomerative clustering, we can merge common features into clusters, while allow us to discard features/columns while retaining much of the information. \n",
    "\n",
    "So, in this implementation, our goal is still to use dimensionality reduction, but this time around, instead of using PCA to find new axes, we use agglomerative clustering to find the features that we should keep. \n",
    "\n",
    "In sklearn, we perform agglomerative clustering on features using the FeatureAgglomeration() object in the cluster module. When initializing the object, n_clusters keyword argument (which represents the number of final clusters) is used to specify the new feature dimension of the data.\n",
    "\n",
    "The code below demonstrates how to use the FeatureAgglomeration object to reduce feature dimensionality from 4 to 2. We use the object's fit_transform function to fit the clustering model on the data, then subsequently apply the feature reduction on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print('Original shape: {}\\n'.format(data.shape))\n",
    "print('First 10:\\n{}\\n'.format(repr(data[:10])))\n",
    "\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "agg = FeatureAgglomeration(n_clusters=2)\n",
    "new_data = agg.fit_transform(data)\n",
    "print('New shape: {}\\n'.format(new_data.shape))\n",
    "print('First 10:\\n{}\\n'.format(repr(new_data[:10])))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Original shape: (150, 4)\n",
    "\n",
    "First 10:\n",
    "array([[5.1, 3.5, 1.4, 0.2],\n",
    "       [4.9, 3. , 1.4, 0.2],\n",
    "       [4.7, 3.2, 1.3, 0.2],\n",
    "       [4.6, 3.1, 1.5, 0.2],\n",
    "       [5. , 3.6, 1.4, 0.2],\n",
    "       [5.4, 3.9, 1.7, 0.4],\n",
    "       [4.6, 3.4, 1.4, 0.3],\n",
    "       [5. , 3.4, 1.5, 0.2],\n",
    "       [4.4, 2.9, 1.4, 0.2],\n",
    "       [4.9, 3.1, 1.5, 0.1]])\n",
    "\n",
    "New shape: (150, 2)\n",
    "\n",
    "First 10:\n",
    "array([[1.7       , 5.1       ],\n",
    "       [1.53333333, 4.9       ],\n",
    "       [1.56666667, 4.7       ],\n",
    "       [1.6       , 4.6       ],\n",
    "       [1.73333333, 5.        ],\n",
    "       [2.        , 5.4       ],\n",
    "       [1.7       , 4.6       ],\n",
    "       [1.7       , 5.        ],\n",
    "       [1.5       , 4.4       ],\n",
    "       [1.56666667, 4.9       ]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: Mean Shift Clustering\n",
    "\n",
    "Mean shift clustering lets us determine the optimal number of clusters. \n",
    "\n",
    "The mean shift algorithm is based on trying to find cluster centroids. It does so by trying to find \"blobs\" in the data, and then determines the centroids from these \"blobs\". The final set of centroids determines the number of clusters as well as the dataset cluster assignments (data observations are assigned to the nearest centroid).\n",
    "\n",
    "In reality, it passes each value through a kernel (Gaussian kernel is a common one). From doing this, the data can be represented as like a \"landscape of hills\", which have peaks. In essence, mean shift clustering imagines what each point would look like if it climbed up to the \"nearest peak\". We can tune the kernel bandwidth to get different #s of clusters, but the algorithm runs until, in essence, the points have hit a \"peak\". \n",
    "\n",
    "The mean shift algorithm is implemented with the MeanShift() object, which is part of the cluster module. It functions like kmeans, except we don't have to specify a certain # of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import MeanShift\n",
    "mean_shift = MeanShift()\n",
    "# predefined data\n",
    "mean_shift.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(mean_shift.labels_)))\n",
    "\n",
    "# centroids\n",
    "print('{}\\n'.format(repr(mean_shift.cluster_centers_)))\n",
    "\n",
    "new_obs = np.array([\n",
    "  [5.1, 3.2, 1.7, 1.9],\n",
    "  [6.9, 3.2, 5.3, 2.2]])\n",
    "# predict clusters\n",
    "print('{}\\n'.format(repr(mean_shift.predict(new_obs))))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT: \n",
    "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "array([[6.21142857, 2.89285714, 4.85285714, 1.67285714],\n",
    "       [5.01632653, 3.44081633, 1.46734694, 0.24285714]])\n",
    "\n",
    "array([1, 0])\n",
    "\n",
    "\"\"\"\n",
    "# since this is a centroid, based algorithm, we can also get the centroids and the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 6: DBSCAN\n",
    "\n",
    "Mean shift clustering is good for determining the number of clusters, but it has two flaws. First, it is not scalable due to computation time. Second, it assumes that the clusters have a \"blob-like\" shape (though the assumption isn't as strong as the assumption made by kmeans)\n",
    "\n",
    "DBSCAN, like mean shift clustering, can automatically choose the number of clusters, but it gets around these two problems. DBSCAN finds dense regions in the dataset, regions with many closely-packed data observations. \n",
    "\n",
    "The DBSCAN algorithm treats high-density regions as clusters in the dataset, and low-density regions as the area between clusters (so observations in the low-density regions are treated as noise and not placed in a cluster).\n",
    "\n",
    "This means that certain observations may not even be placed in a cluster. The DBSCAN algorithm cares about maintaining the integrity of the observations that it does cluster. \n",
    "\n",
    "High density regions have \"core samples\", or observations with many neighbors. Each cluster has multiple core samples and all observations in the cluster are neighbors to a core sample. \n",
    "\n",
    "The way we define \"neighbor\" and \"core sample\" depend on what we want. We can tune a parameter, $\\epsilon$, that determines the maximum distance between two observations that we'll consider as neighbors. We also specify the minimum number of points in the neighborhood of a data observation for the observation to be considered a core sample (the neighborhood consists of the data observation and all its neighbors)\n",
    "\n",
    "In sklearn, DBSCAN is implemented with the DBSCAN object, as a part of the cluster module. The object is initialized with the keyword arguments eps (representing the value of ε) and min_samples (representing the minimum size of a core sample's neighborhood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOUTPUT:\\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\\n\\narray([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\\n        13,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\\n        55,  56,  58,  59,  61,  62,  63,  64,  65,  66,  67,  68,  69,\\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\\n        83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  94,  95,  96,\\n        97,  99, 101, 102, 103, 104, 108, 110, 111, 112, 113, 114, 115,\\n       116, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 132, 133,\\n       134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\\n       148, 149])\\n\\nNum core samples: 132\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=30)\n",
    "# predefined data\n",
    "dbscan.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(dbscan.labels_)))\n",
    "\n",
    "# core samples\n",
    "print('{}\\n'.format(repr(dbscan.core_sample_indices_)))\n",
    "num_core_samples = len(dbscan.core_sample_indices_)\n",
    "print('Num core samples: {}\\n'.format(num_core_samples))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
    "\n",
    "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
    "        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
    "        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
    "        55,  56,  58,  59,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
    "        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
    "        83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  94,  95,  96,\n",
    "        97,  99, 101, 102, 103, 104, 108, 110, 111, 112, 113, 114, 115,\n",
    "       116, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 132, 133,\n",
    "       134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
    "       148, 149])\n",
    "\n",
    "Num core samples: 132\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we used DBSCAN to cluster the 150 data observations in data. The algorithm found two clusters. In this case all the data observations fit in a cluster, but in general the non-cluster observations would be labeled with -1.\n",
    "\n",
    "The core_sample_indices_ attribute represents the core sample data observations in data (specified by row index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we evaluate clustering algorithms?\n",
    "\n",
    "One popular metric to evaluate the performance of clustering algorithms is the adjusted Rand index. The regular Rand index gives a measurement of the similarity between the true clustering assignments (labels) and the predicted clustering assignments (predicted). The adjusted Rand index (ARI) modifies this by correcting for chance results, so that random clustering assignments will not have a good score.\n",
    "\n",
    "The ARI value ranges from -1 to 1, inclusive. Negative scores represent bad labelings, random labelings will get a score near 0, and perfect labelings get a score of 1.\n",
    "\n",
    "In sklearn, the ARI is implemented with the adjusted_rand_score function, which is part of the metrics module. It takes in two required arguments, the true cluster labels and the predicted cluster labels, and returns the ARI score.\n",
    "\n",
    "Note that the adjusted_rand_score function is symmetric. This means that changing the order of the arguments will not affect the score. Furthermore, permutations in the labeling or changing the label names (i.e. 0 and 1 vs. 1 and 3) does not affect the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24242424242424246\n",
      "\n",
      "0.24242424242424246\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "-0.12903225806451613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "true_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "pred_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# symmetric\n",
    "ari = adjusted_rand_score(pred_labels, true_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# Perfect labeling\n",
    "perf_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "ari = adjusted_rand_score(true_labels, perf_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# Perfect labeling, permuted\n",
    "permuted_labels = np.array([1, 1, 1, 0, 0, 0])\n",
    "ari = adjusted_rand_score(true_labels, permuted_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "renamed_labels = np.array([1, 1, 1, 3, 3, 3])\n",
    "# Renamed labels to 1, 3\n",
    "ari = adjusted_rand_score(true_labels, renamed_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "true_labels2 = np.array([0, 1, 2, 0, 3, 4, 5, 1])\n",
    "# Bad labeling\n",
    "pred_labels2 = np.array([1, 1, 0, 0, 2, 2, 2, 2])\n",
    "ari = adjusted_rand_score(true_labels2, pred_labels2)\n",
    "print('{}\\n'.format(ari))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common metric is adjusted mutual information (AMI). In sklearn, it is implemented with the adjusted_mutual_info_score function, also a part of the cluster module. Like adjusted_rand_score, the function is symmetric and oblivious to permutations and renamed labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2250422831983088\n",
      "\n",
      "0.2250422831983088\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/Users/mark/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/Users/mark/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/Users/mark/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/Users/mark/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "true_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "pred_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "ami = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# symmetric\n",
    "ami = adjusted_mutual_info_score(pred_labels, true_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# Perfect labeling\n",
    "perf_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "ami = adjusted_mutual_info_score(true_labels, perf_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# Perfect labeling, permuted\n",
    "permuted_labels = np.array([1, 1, 1, 0, 0, 0])\n",
    "ami = adjusted_mutual_info_score(true_labels, permuted_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "renamed_labels = np.array([1, 1, 1, 3, 3, 3])\n",
    "# Renamed labels to 1, 3\n",
    "ami = adjusted_mutual_info_score(true_labels, renamed_labels)\n",
    "print('{}\\n'.format(ami))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARI and AMI metrics are very similar. They both assign a score of 1.0 to perfect labelings, a score near 0.0 to random labelings, and negative scores to poor labelings.\n",
    "\n",
    "A general rule of thumb of when to use which: ARI is used when the true clusters are large and approximately equal sized, while AMI is used when the true clusters are unbalanced in size and there exist small clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
